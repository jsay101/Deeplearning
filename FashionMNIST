The FashionMNIST dataset
FashionMNIST, a dataset of article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. For details, check https://github.com/zalandoresearch/fashion-mnist
To reduce the training time, we'll sample:
8000 images from training set for training our model
2000 images from training set for evaluating our model during training
2000 images from testing set to test the performance of our trained model
See code below to load and explore the dataset. All subsets are torch Dataset objects.
Each image has a dimension of (1, 28, 28). Since we'll build a sequential model with linear layers, we'll flatten the image to a vector later.
Each image has a unique label between 0-9.
[12]
pip install torch-summary
Collecting torch-summary
  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)
Installing collected packages: torch-summary
Successfully installed torch-summary-1.4.5
Note: you may need to restart the kernel to use updated packages.

[1]
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import pandas as pd
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import DataLoader, random_split, Subset
import torchvision.transforms as transforms

import torchvision
from torchvision import models
from torchsummary import summary

from numpy.random import seed
import torch
import numpy as np

from torchinfo import summary
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
[2]
# Load dataset

train_dataset = torchvision.datasets.FashionMNIST('./data',
                                           train=True,
                                           transform=transforms.ToTensor(),
                                           download=True)

test_dataset = torchvision.datasets.FashionMNIST('./data',
                                          train=False,
                                          transform=transforms.ToTensor(),
                                          download=True)
[3]
# Randomly select 8000 for training, 2000 for evaluating, 2000 for testing

# randomly select 10000 ids from the the downloaded train_dataset
selected_indexes = np.random.choice(len(train_dataset), 10000, replace=False)

# train subset
train_data = Subset(train_dataset, selected_indexes[0:8000])

# validation subset
val_data = Subset(train_dataset, selected_indexes[8000:])

# test subset
test_data = Subset(test_dataset, np.random.choice(len(test_dataset), 2000, replace=False))

print(f'Train dataset size: {len(train_data)}')
print(f'Validation dataset length: {len(val_data)}')
print(f'Test dataset length: {len(val_data)}')

Train dataset size: 8000
Validation dataset length: 2000
Test dataset length: 2000

[5]
# Show some images from test dataset

batch_size = 32

test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
imgs, labels = next(iter(test_loader))
plt.figure(figsize=(15,10))
grid = torchvision.utils.make_grid(nrow=16, ncols=2, tensor=imgs)
print(f"image tensor minibatch size: {imgs.size()}")
print(f"class labels size: {labels.size()}, labels: {labels}")
plt.imshow(np.transpose(grid, axes=(1,2,0)), cmap='gray');
image tensor minibatch size: torch.Size([32, 1, 28, 28])
class labels size: torch.Size([32]), labels: tensor([5, 9, 4, 0, 2, 5, 7, 0, 5, 5, 8, 3, 6, 4, 7, 3, 2, 8, 5, 7, 6, 7, 6, 9,
        0, 5, 2, 5, 9, 1, 6, 4])


[4]
# Fix random number so that your traing process can be replicated

random_seed = 124
seed(random_seed)

torch.manual_seed(random_seed)
torch.backends.cudnn.deterministic = True
<torch._C.Generator at 0x1a75b43b090>
[5]
class linear_model(nn.Module):
    def __init__(self,input_dim,hidden_units):
        super(linear_model, self).__init__()
        self.classifier=nn.Sequential(
            nn.Linear(in_features=input_dim, out_features=hidden_units[0]),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units[0], out_features=hidden_units[1])
        )
    def forward(self,x):
        return self.classifier(x)
        
        
    
    # add your code here
[6]
def train_model(model, train_dataset, val_dataset, test_dataset, device, 
                optimizer, epochs=30, batch_size=128):
    train_l=DataLoader(train_dataset, batch_size=batch_size)
    eval_l=DataLoader(val_dataset, batch_size=batch_size)
    test_l=DataLoader(test_dataset, batch_size=batch_size)
    
    loss_fn=nn.CrossEntropyLoss()
    optimizer = optimizer
    history = {'eval_loss': [],
            'eval_acc': [],}
    
    for i in range(epochs):
        model.train()
        train_loss = 0
        train_acc = 0
        for x, y in train_l:
            x = x.to(device)
            y = y.to(device)
            x=x.view(x.shape[0],-1)
            output=model(x)
            loss=loss_fn(output,y)
            loss.backward()
            train_loss += loss.item()* x.size(0)
            _, predicted = torch.max(output.data, 1)
            cur_train_acc = (predicted == y).sum().item()
            train_acc += cur_train_acc
            optimizer.step()
            optimizer.zero_grad()
        train_loss /= len(train_dataset)
        

            
        model.eval()
        eval_loss=0
        eval_acc=0
        with torch.no_grad():
            for x,y in eval_l:
                x = x.to(device)
                y = y.to(device)
                x=x.view(x.shape[0],-1)
                output=model(x)
                loss=loss_fn(output,y)
                eval_loss += loss.item()* x.size(0)
                _, predicted = torch.max(output.data, 1)
                cur_eval_acc = (predicted == y).sum().item()
                eval_acc += cur_eval_acc
        eval_loss /= len(val_dataset)
        print(f'Epoch: {i+1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc / len(train_dataset):.4f}, Validation Loss: {eval_loss:.4f}, Validation Accuracy: {eval_acc / len(val_dataset):.4f}')
        
        ##test
        model.eval()
        test_loss=0
        test_acc=0
        with torch.no_grad():
            for x,y in test_l:
                x= x.to(device)
                y = y.to(device)
                x=x.view(x.shape[0],-1)
                output=model(x)
                loss=loss_fn(output,y)
                test_loss += loss.item()* x.size(0)
                _, predicted = torch.max(output.data, 1)
                cur_test_acc = (predicted == y).sum().item()
                test_acc += cur_test_acc
        test_loss /= len(test_dataset)
        history['eval_loss'].append(eval_loss)
        history['eval_acc'].append(eval_acc / len(val_dataset))
            
                
    
    
    
    # add your code
    

    return test_acc / len(test_dataset), history
[7]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
[8]
# dataset

modela = linear_model(input_dim = 784, hidden_units=[64,  10])
#summary(modela, (32, 784))

optimizer = torch.optim.RMSprop(modela.parameters(), lr = 0.0005)
acc_a, hista = train_model(modela, train_data, val_data, test_data, device, optimizer, epochs=100)
Epoch: 1, Training Loss: 1.0713, Training Accuracy: 0.6607, Validation Loss: 0.7609, Validation Accuracy: 0.7460
Epoch: 2, Training Loss: 0.7146, Training Accuracy: 0.7572, Validation Loss: 0.6555, Validation Accuracy: 0.7780
Epoch: 3, Training Loss: 0.6334, Training Accuracy: 0.7839, Validation Loss: 0.6069, Validation Accuracy: 0.7910
Epoch: 4, Training Loss: 0.5848, Training Accuracy: 0.8034, Validation Loss: 0.5750, Validation Accuracy: 0.7980
Epoch: 5, Training Loss: 0.5507, Training Accuracy: 0.8133, Validation Loss: 0.5541, Validation Accuracy: 0.8040
Epoch: 6, Training Loss: 0.5248, Training Accuracy: 0.8217, Validation Loss: 0.5379, Validation Accuracy: 0.8090
Epoch: 7, Training Loss: 0.5042, Training Accuracy: 0.8291, Validation Loss: 0.5257, Validation Accuracy: 0.8105
Epoch: 8, Training Loss: 0.4867, Training Accuracy: 0.8335, Validation Loss: 0.5150, Validation Accuracy: 0.8110
Epoch: 9, Training Loss: 0.4719, Training Accuracy: 0.8386, Validation Loss: 0.5065, Validation Accuracy: 0.8120
Epoch: 10, Training Loss: 0.4590, Training Accuracy: 0.8424, Validation Loss: 0.4991, Validation Accuracy: 0.8155
Epoch: 11, Training Loss: 0.4478, Training Accuracy: 0.8465, Validation Loss: 0.4923, Validation Accuracy: 0.8190
Epoch: 12, Training Loss: 0.4377, Training Accuracy: 0.8502, Validation Loss: 0.4861, Validation Accuracy: 0.8220
Epoch: 13, Training Loss: 0.4284, Training Accuracy: 0.8515, Validation Loss: 0.4814, Validation Accuracy: 0.8225
Epoch: 14, Training Loss: 0.4200, Training Accuracy: 0.8552, Validation Loss: 0.4761, Validation Accuracy: 0.8260
Epoch: 15, Training Loss: 0.4121, Training Accuracy: 0.8575, Validation Loss: 0.4722, Validation Accuracy: 0.8270
Epoch: 16, Training Loss: 0.4049, Training Accuracy: 0.8608, Validation Loss: 0.4681, Validation Accuracy: 0.8285
Epoch: 17, Training Loss: 0.3979, Training Accuracy: 0.8636, Validation Loss: 0.4649, Validation Accuracy: 0.8315
Epoch: 18, Training Loss: 0.3914, Training Accuracy: 0.8660, Validation Loss: 0.4620, Validation Accuracy: 0.8350
Epoch: 19, Training Loss: 0.3853, Training Accuracy: 0.8686, Validation Loss: 0.4600, Validation Accuracy: 0.8340
Epoch: 20, Training Loss: 0.3795, Training Accuracy: 0.8705, Validation Loss: 0.4568, Validation Accuracy: 0.8350
Epoch: 21, Training Loss: 0.3738, Training Accuracy: 0.8726, Validation Loss: 0.4543, Validation Accuracy: 0.8375
Epoch: 22, Training Loss: 0.3684, Training Accuracy: 0.8736, Validation Loss: 0.4521, Validation Accuracy: 0.8375
Epoch: 23, Training Loss: 0.3631, Training Accuracy: 0.8755, Validation Loss: 0.4504, Validation Accuracy: 0.8380
Epoch: 24, Training Loss: 0.3579, Training Accuracy: 0.8764, Validation Loss: 0.4487, Validation Accuracy: 0.8390
Epoch: 25, Training Loss: 0.3529, Training Accuracy: 0.8788, Validation Loss: 0.4474, Validation Accuracy: 0.8390
Epoch: 26, Training Loss: 0.3481, Training Accuracy: 0.8812, Validation Loss: 0.4449, Validation Accuracy: 0.8400
Epoch: 27, Training Loss: 0.3434, Training Accuracy: 0.8821, Validation Loss: 0.4435, Validation Accuracy: 0.8390
Epoch: 28, Training Loss: 0.3389, Training Accuracy: 0.8841, Validation Loss: 0.4409, Validation Accuracy: 0.8370
Epoch: 29, Training Loss: 0.3345, Training Accuracy: 0.8856, Validation Loss: 0.4393, Validation Accuracy: 0.8375
Epoch: 30, Training Loss: 0.3302, Training Accuracy: 0.8870, Validation Loss: 0.4380, Validation Accuracy: 0.8400
Epoch: 31, Training Loss: 0.3261, Training Accuracy: 0.8876, Validation Loss: 0.4369, Validation Accuracy: 0.8390
Epoch: 32, Training Loss: 0.3220, Training Accuracy: 0.8884, Validation Loss: 0.4357, Validation Accuracy: 0.8395
Epoch: 33, Training Loss: 0.3180, Training Accuracy: 0.8894, Validation Loss: 0.4343, Validation Accuracy: 0.8400
Epoch: 34, Training Loss: 0.3142, Training Accuracy: 0.8908, Validation Loss: 0.4330, Validation Accuracy: 0.8400
Epoch: 35, Training Loss: 0.3103, Training Accuracy: 0.8920, Validation Loss: 0.4317, Validation Accuracy: 0.8420
Epoch: 36, Training Loss: 0.3066, Training Accuracy: 0.8929, Validation Loss: 0.4310, Validation Accuracy: 0.8435
Epoch: 37, Training Loss: 0.3030, Training Accuracy: 0.8942, Validation Loss: 0.4301, Validation Accuracy: 0.8440
Epoch: 38, Training Loss: 0.2995, Training Accuracy: 0.8959, Validation Loss: 0.4296, Validation Accuracy: 0.8465
Epoch: 39, Training Loss: 0.2959, Training Accuracy: 0.8964, Validation Loss: 0.4275, Validation Accuracy: 0.8485
Epoch: 40, Training Loss: 0.2927, Training Accuracy: 0.8974, Validation Loss: 0.4271, Validation Accuracy: 0.8495
Epoch: 41, Training Loss: 0.2892, Training Accuracy: 0.8989, Validation Loss: 0.4272, Validation Accuracy: 0.8480
Epoch: 42, Training Loss: 0.2859, Training Accuracy: 0.8991, Validation Loss: 0.4261, Validation Accuracy: 0.8495
Epoch: 43, Training Loss: 0.2828, Training Accuracy: 0.9009, Validation Loss: 0.4267, Validation Accuracy: 0.8490
Epoch: 44, Training Loss: 0.2796, Training Accuracy: 0.9020, Validation Loss: 0.4246, Validation Accuracy: 0.8505
Epoch: 45, Training Loss: 0.2766, Training Accuracy: 0.9026, Validation Loss: 0.4256, Validation Accuracy: 0.8495
Epoch: 46, Training Loss: 0.2735, Training Accuracy: 0.9044, Validation Loss: 0.4259, Validation Accuracy: 0.8490
Epoch: 47, Training Loss: 0.2706, Training Accuracy: 0.9052, Validation Loss: 0.4259, Validation Accuracy: 0.8490
Epoch: 48, Training Loss: 0.2677, Training Accuracy: 0.9058, Validation Loss: 0.4244, Validation Accuracy: 0.8505
Epoch: 49, Training Loss: 0.2649, Training Accuracy: 0.9075, Validation Loss: 0.4260, Validation Accuracy: 0.8495
Epoch: 50, Training Loss: 0.2621, Training Accuracy: 0.9083, Validation Loss: 0.4244, Validation Accuracy: 0.8500
Epoch: 51, Training Loss: 0.2594, Training Accuracy: 0.9097, Validation Loss: 0.4263, Validation Accuracy: 0.8495
Epoch: 52, Training Loss: 0.2566, Training Accuracy: 0.9115, Validation Loss: 0.4263, Validation Accuracy: 0.8495
Epoch: 53, Training Loss: 0.2539, Training Accuracy: 0.9123, Validation Loss: 0.4268, Validation Accuracy: 0.8500
Epoch: 54, Training Loss: 0.2513, Training Accuracy: 0.9133, Validation Loss: 0.4256, Validation Accuracy: 0.8525
Epoch: 55, Training Loss: 0.2488, Training Accuracy: 0.9151, Validation Loss: 0.4274, Validation Accuracy: 0.8505
Epoch: 56, Training Loss: 0.2464, Training Accuracy: 0.9153, Validation Loss: 0.4279, Validation Accuracy: 0.8500
Epoch: 57, Training Loss: 0.2438, Training Accuracy: 0.9165, Validation Loss: 0.4285, Validation Accuracy: 0.8510
Epoch: 58, Training Loss: 0.2413, Training Accuracy: 0.9180, Validation Loss: 0.4268, Validation Accuracy: 0.8515
Epoch: 59, Training Loss: 0.2390, Training Accuracy: 0.9183, Validation Loss: 0.4297, Validation Accuracy: 0.8530
Epoch: 60, Training Loss: 0.2366, Training Accuracy: 0.9194, Validation Loss: 0.4302, Validation Accuracy: 0.8530
Epoch: 61, Training Loss: 0.2344, Training Accuracy: 0.9204, Validation Loss: 0.4295, Validation Accuracy: 0.8525
Epoch: 62, Training Loss: 0.2320, Training Accuracy: 0.9210, Validation Loss: 0.4299, Validation Accuracy: 0.8525
Epoch: 63, Training Loss: 0.2297, Training Accuracy: 0.9219, Validation Loss: 0.4309, Validation Accuracy: 0.8520
Epoch: 64, Training Loss: 0.2275, Training Accuracy: 0.9224, Validation Loss: 0.4315, Validation Accuracy: 0.8515
Epoch: 65, Training Loss: 0.2252, Training Accuracy: 0.9231, Validation Loss: 0.4323, Validation Accuracy: 0.8515
Epoch: 66, Training Loss: 0.2231, Training Accuracy: 0.9241, Validation Loss: 0.4329, Validation Accuracy: 0.8525
Epoch: 67, Training Loss: 0.2208, Training Accuracy: 0.9259, Validation Loss: 0.4329, Validation Accuracy: 0.8525
Epoch: 68, Training Loss: 0.2188, Training Accuracy: 0.9257, Validation Loss: 0.4332, Validation Accuracy: 0.8530
Epoch: 69, Training Loss: 0.2167, Training Accuracy: 0.9266, Validation Loss: 0.4333, Validation Accuracy: 0.8540
Epoch: 70, Training Loss: 0.2147, Training Accuracy: 0.9271, Validation Loss: 0.4346, Validation Accuracy: 0.8555
Epoch: 71, Training Loss: 0.2125, Training Accuracy: 0.9275, Validation Loss: 0.4347, Validation Accuracy: 0.8555
Epoch: 72, Training Loss: 0.2107, Training Accuracy: 0.9280, Validation Loss: 0.4361, Validation Accuracy: 0.8565
Epoch: 73, Training Loss: 0.2087, Training Accuracy: 0.9284, Validation Loss: 0.4371, Validation Accuracy: 0.8555
Epoch: 74, Training Loss: 0.2068, Training Accuracy: 0.9291, Validation Loss: 0.4366, Validation Accuracy: 0.8560
Epoch: 75, Training Loss: 0.2046, Training Accuracy: 0.9307, Validation Loss: 0.4368, Validation Accuracy: 0.8560
Epoch: 76, Training Loss: 0.2029, Training Accuracy: 0.9309, Validation Loss: 0.4388, Validation Accuracy: 0.8550
Epoch: 77, Training Loss: 0.2011, Training Accuracy: 0.9315, Validation Loss: 0.4402, Validation Accuracy: 0.8565
Epoch: 78, Training Loss: 0.1993, Training Accuracy: 0.9323, Validation Loss: 0.4406, Validation Accuracy: 0.8555
Epoch: 79, Training Loss: 0.1975, Training Accuracy: 0.9329, Validation Loss: 0.4417, Validation Accuracy: 0.8560
Epoch: 80, Training Loss: 0.1956, Training Accuracy: 0.9331, Validation Loss: 0.4443, Validation Accuracy: 0.8570
Epoch: 81, Training Loss: 0.1938, Training Accuracy: 0.9340, Validation Loss: 0.4440, Validation Accuracy: 0.8560
Epoch: 82, Training Loss: 0.1921, Training Accuracy: 0.9349, Validation Loss: 0.4459, Validation Accuracy: 0.8565
Epoch: 83, Training Loss: 0.1905, Training Accuracy: 0.9353, Validation Loss: 0.4464, Validation Accuracy: 0.8560
Epoch: 84, Training Loss: 0.1886, Training Accuracy: 0.9361, Validation Loss: 0.4474, Validation Accuracy: 0.8565
Epoch: 85, Training Loss: 0.1870, Training Accuracy: 0.9365, Validation Loss: 0.4477, Validation Accuracy: 0.8560
Epoch: 86, Training Loss: 0.1855, Training Accuracy: 0.9361, Validation Loss: 0.4494, Validation Accuracy: 0.8570
Epoch: 87, Training Loss: 0.1838, Training Accuracy: 0.9376, Validation Loss: 0.4512, Validation Accuracy: 0.8560
Epoch: 88, Training Loss: 0.1821, Training Accuracy: 0.9377, Validation Loss: 0.4516, Validation Accuracy: 0.8555
Epoch: 89, Training Loss: 0.1805, Training Accuracy: 0.9400, Validation Loss: 0.4527, Validation Accuracy: 0.8560
Epoch: 90, Training Loss: 0.1788, Training Accuracy: 0.9401, Validation Loss: 0.4531, Validation Accuracy: 0.8560
Epoch: 91, Training Loss: 0.1774, Training Accuracy: 0.9417, Validation Loss: 0.4552, Validation Accuracy: 0.8560
Epoch: 92, Training Loss: 0.1757, Training Accuracy: 0.9423, Validation Loss: 0.4563, Validation Accuracy: 0.8550
Epoch: 93, Training Loss: 0.1742, Training Accuracy: 0.9431, Validation Loss: 0.4580, Validation Accuracy: 0.8545
Epoch: 94, Training Loss: 0.1727, Training Accuracy: 0.9433, Validation Loss: 0.4592, Validation Accuracy: 0.8550
Epoch: 95, Training Loss: 0.1710, Training Accuracy: 0.9440, Validation Loss: 0.4602, Validation Accuracy: 0.8540
Epoch: 96, Training Loss: 0.1696, Training Accuracy: 0.9444, Validation Loss: 0.4618, Validation Accuracy: 0.8555
Epoch: 97, Training Loss: 0.1682, Training Accuracy: 0.9453, Validation Loss: 0.4632, Validation Accuracy: 0.8535
Epoch: 98, Training Loss: 0.1667, Training Accuracy: 0.9457, Validation Loss: 0.4641, Validation Accuracy: 0.8555
Epoch: 99, Training Loss: 0.1652, Training Accuracy: 0.9455, Validation Loss: 0.4653, Validation Accuracy: 0.8540
Epoch: 100, Training Loss: 0.1637, Training Accuracy: 0.9470, Validation Loss: 0.4672, Validation Accuracy: 0.8545

[25]
acc_a
0.838
[31]
class linear_model_B(nn.Module):
    def __init__(self,input_dim,hidden_units):
        super(linear_model_B, self).__init__()
        self.classifier=nn.Sequential(
            nn.Linear(in_features=input_dim, out_features=hidden_units[0]),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units[0], out_features=hidden_units[1]),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units[1], out_features=hidden_units[2]),
        )
        
    def forward(self,x):
        return self.classifier(x)

[32]
# dataset

modelb = linear_model_B(input_dim = 784, hidden_units=[256,128,10])
#summary(modela, (32, 784))

optimizer = torch.optim.RMSprop(modelb.parameters(), lr = 0.0005)
acc_b, histb = train_model(modelb, train_data, val_data, test_data, device, optimizer, epochs=100)

acc_b
Epoch: 1, Training Loss: 0.9858, Training Accuracy: 0.6356, Validation Loss: 0.6873, Validation Accuracy: 0.7330
Epoch: 2, Training Loss: 0.6332, Training Accuracy: 0.7662, Validation Loss: 0.5876, Validation Accuracy: 0.7845
Epoch: 3, Training Loss: 0.5575, Training Accuracy: 0.7994, Validation Loss: 0.5493, Validation Accuracy: 0.7990
Epoch: 4, Training Loss: 0.5127, Training Accuracy: 0.8149, Validation Loss: 0.5258, Validation Accuracy: 0.8055
Epoch: 5, Training Loss: 0.4797, Training Accuracy: 0.8275, Validation Loss: 0.5101, Validation Accuracy: 0.8115
Epoch: 6, Training Loss: 0.4527, Training Accuracy: 0.8364, Validation Loss: 0.4991, Validation Accuracy: 0.8165
Epoch: 7, Training Loss: 0.4304, Training Accuracy: 0.8460, Validation Loss: 0.4860, Validation Accuracy: 0.8250
Epoch: 8, Training Loss: 0.4109, Training Accuracy: 0.8540, Validation Loss: 0.4752, Validation Accuracy: 0.8265
Epoch: 9, Training Loss: 0.3934, Training Accuracy: 0.8588, Validation Loss: 0.4662, Validation Accuracy: 0.8315
Epoch: 10, Training Loss: 0.3779, Training Accuracy: 0.8660, Validation Loss: 0.4642, Validation Accuracy: 0.8325
Epoch: 11, Training Loss: 0.3640, Training Accuracy: 0.8721, Validation Loss: 0.4526, Validation Accuracy: 0.8330
Epoch: 12, Training Loss: 0.3500, Training Accuracy: 0.8771, Validation Loss: 0.4501, Validation Accuracy: 0.8340
Epoch: 13, Training Loss: 0.3373, Training Accuracy: 0.8829, Validation Loss: 0.4464, Validation Accuracy: 0.8350
Epoch: 14, Training Loss: 0.3254, Training Accuracy: 0.8861, Validation Loss: 0.4447, Validation Accuracy: 0.8385
Epoch: 15, Training Loss: 0.3144, Training Accuracy: 0.8904, Validation Loss: 0.4429, Validation Accuracy: 0.8390
Epoch: 16, Training Loss: 0.3036, Training Accuracy: 0.8940, Validation Loss: 0.4424, Validation Accuracy: 0.8375
Epoch: 17, Training Loss: 0.2935, Training Accuracy: 0.8974, Validation Loss: 0.4396, Validation Accuracy: 0.8405
Epoch: 18, Training Loss: 0.2834, Training Accuracy: 0.9010, Validation Loss: 0.4400, Validation Accuracy: 0.8390
Epoch: 19, Training Loss: 0.2739, Training Accuracy: 0.9045, Validation Loss: 0.4386, Validation Accuracy: 0.8410
Epoch: 20, Training Loss: 0.2652, Training Accuracy: 0.9077, Validation Loss: 0.4402, Validation Accuracy: 0.8390
Epoch: 21, Training Loss: 0.2561, Training Accuracy: 0.9107, Validation Loss: 0.4402, Validation Accuracy: 0.8420
Epoch: 22, Training Loss: 0.2470, Training Accuracy: 0.9133, Validation Loss: 0.4420, Validation Accuracy: 0.8410
Epoch: 23, Training Loss: 0.2389, Training Accuracy: 0.9156, Validation Loss: 0.4413, Validation Accuracy: 0.8420
Epoch: 24, Training Loss: 0.2313, Training Accuracy: 0.9183, Validation Loss: 0.4424, Validation Accuracy: 0.8435
Epoch: 25, Training Loss: 0.2227, Training Accuracy: 0.9209, Validation Loss: 0.4453, Validation Accuracy: 0.8420
Epoch: 26, Training Loss: 0.2145, Training Accuracy: 0.9243, Validation Loss: 0.4441, Validation Accuracy: 0.8465
Epoch: 27, Training Loss: 0.2085, Training Accuracy: 0.9271, Validation Loss: 0.4472, Validation Accuracy: 0.8470
Epoch: 28, Training Loss: 0.2002, Training Accuracy: 0.9289, Validation Loss: 0.4506, Validation Accuracy: 0.8470
Epoch: 29, Training Loss: 0.1934, Training Accuracy: 0.9319, Validation Loss: 0.4504, Validation Accuracy: 0.8480
Epoch: 30, Training Loss: 0.1879, Training Accuracy: 0.9347, Validation Loss: 0.4485, Validation Accuracy: 0.8510
Epoch: 31, Training Loss: 0.1785, Training Accuracy: 0.9371, Validation Loss: 0.4612, Validation Accuracy: 0.8445
Epoch: 32, Training Loss: 0.1773, Training Accuracy: 0.9364, Validation Loss: 0.4742, Validation Accuracy: 0.8450
Epoch: 33, Training Loss: 0.1687, Training Accuracy: 0.9417, Validation Loss: 0.4549, Validation Accuracy: 0.8515
Epoch: 34, Training Loss: 0.1607, Training Accuracy: 0.9446, Validation Loss: 0.4615, Validation Accuracy: 0.8510
Epoch: 35, Training Loss: 0.1564, Training Accuracy: 0.9457, Validation Loss: 0.4757, Validation Accuracy: 0.8575
Epoch: 36, Training Loss: 0.1533, Training Accuracy: 0.9476, Validation Loss: 0.6431, Validation Accuracy: 0.8295
Epoch: 37, Training Loss: 0.1482, Training Accuracy: 0.9494, Validation Loss: 0.4989, Validation Accuracy: 0.8500
Epoch: 38, Training Loss: 0.1404, Training Accuracy: 0.9526, Validation Loss: 0.4844, Validation Accuracy: 0.8540
Epoch: 39, Training Loss: 0.1390, Training Accuracy: 0.9530, Validation Loss: 0.4774, Validation Accuracy: 0.8560
Epoch: 40, Training Loss: 0.1308, Training Accuracy: 0.9559, Validation Loss: 0.4913, Validation Accuracy: 0.8545
Epoch: 41, Training Loss: 0.1459, Training Accuracy: 0.9531, Validation Loss: 0.5789, Validation Accuracy: 0.8385
Epoch: 42, Training Loss: 0.1208, Training Accuracy: 0.9607, Validation Loss: 0.5498, Validation Accuracy: 0.8480
Epoch: 43, Training Loss: 0.1260, Training Accuracy: 0.9580, Validation Loss: 0.5176, Validation Accuracy: 0.8520
Epoch: 44, Training Loss: 0.1130, Training Accuracy: 0.9634, Validation Loss: 0.5128, Validation Accuracy: 0.8540
Epoch: 45, Training Loss: 0.1133, Training Accuracy: 0.9617, Validation Loss: 0.5113, Validation Accuracy: 0.8510
Epoch: 46, Training Loss: 0.1023, Training Accuracy: 0.9686, Validation Loss: 0.6262, Validation Accuracy: 0.8420
Epoch: 47, Training Loss: 0.1090, Training Accuracy: 0.9649, Validation Loss: 0.5396, Validation Accuracy: 0.8515
Epoch: 48, Training Loss: 0.1018, Training Accuracy: 0.9677, Validation Loss: 0.5201, Validation Accuracy: 0.8545
Epoch: 49, Training Loss: 0.0951, Training Accuracy: 0.9701, Validation Loss: 0.5231, Validation Accuracy: 0.8545
Epoch: 50, Training Loss: 0.1006, Training Accuracy: 0.9680, Validation Loss: 0.5284, Validation Accuracy: 0.8580
Epoch: 51, Training Loss: 0.0861, Training Accuracy: 0.9726, Validation Loss: 0.5594, Validation Accuracy: 0.8530
Epoch: 52, Training Loss: 0.0873, Training Accuracy: 0.9735, Validation Loss: 0.6734, Validation Accuracy: 0.8410
Epoch: 53, Training Loss: 0.0891, Training Accuracy: 0.9711, Validation Loss: 0.5581, Validation Accuracy: 0.8525
Epoch: 54, Training Loss: 0.0884, Training Accuracy: 0.9720, Validation Loss: 0.5613, Validation Accuracy: 0.8565
Epoch: 55, Training Loss: 0.0857, Training Accuracy: 0.9735, Validation Loss: 0.5667, Validation Accuracy: 0.8550
Epoch: 56, Training Loss: 0.0688, Training Accuracy: 0.9796, Validation Loss: 0.5635, Validation Accuracy: 0.8565
Epoch: 57, Training Loss: 0.0844, Training Accuracy: 0.9726, Validation Loss: 0.5704, Validation Accuracy: 0.8525
Epoch: 58, Training Loss: 0.0659, Training Accuracy: 0.9799, Validation Loss: 0.6067, Validation Accuracy: 0.8500
Epoch: 59, Training Loss: 0.0636, Training Accuracy: 0.9808, Validation Loss: 0.6280, Validation Accuracy: 0.8495
Epoch: 60, Training Loss: 0.1104, Training Accuracy: 0.9709, Validation Loss: 0.5932, Validation Accuracy: 0.8550
Epoch: 61, Training Loss: 0.0555, Training Accuracy: 0.9859, Validation Loss: 0.5999, Validation Accuracy: 0.8575
Epoch: 62, Training Loss: 0.0569, Training Accuracy: 0.9824, Validation Loss: 0.6134, Validation Accuracy: 0.8530
Epoch: 63, Training Loss: 0.0780, Training Accuracy: 0.9765, Validation Loss: 0.7774, Validation Accuracy: 0.8265
Epoch: 64, Training Loss: 0.0652, Training Accuracy: 0.9799, Validation Loss: 0.6119, Validation Accuracy: 0.8495
Epoch: 65, Training Loss: 0.0591, Training Accuracy: 0.9801, Validation Loss: 0.7717, Validation Accuracy: 0.8320
Epoch: 66, Training Loss: 0.0537, Training Accuracy: 0.9836, Validation Loss: 0.6150, Validation Accuracy: 0.8535
Epoch: 67, Training Loss: 0.0490, Training Accuracy: 0.9872, Validation Loss: 0.6454, Validation Accuracy: 0.8560
Epoch: 68, Training Loss: 0.0739, Training Accuracy: 0.9780, Validation Loss: 0.6305, Validation Accuracy: 0.8505
Epoch: 69, Training Loss: 0.0403, Training Accuracy: 0.9899, Validation Loss: 0.6329, Validation Accuracy: 0.8525
Epoch: 70, Training Loss: 0.0870, Training Accuracy: 0.9782, Validation Loss: 0.6251, Validation Accuracy: 0.8590
Epoch: 71, Training Loss: 0.0378, Training Accuracy: 0.9896, Validation Loss: 0.6527, Validation Accuracy: 0.8565
Epoch: 72, Training Loss: 0.1200, Training Accuracy: 0.9724, Validation Loss: 0.6019, Validation Accuracy: 0.8605
Epoch: 73, Training Loss: 0.0386, Training Accuracy: 0.9892, Validation Loss: 0.6493, Validation Accuracy: 0.8580
Epoch: 74, Training Loss: 0.0342, Training Accuracy: 0.9915, Validation Loss: 0.6631, Validation Accuracy: 0.8595
Epoch: 75, Training Loss: 0.0655, Training Accuracy: 0.9830, Validation Loss: 0.6211, Validation Accuracy: 0.8600
Epoch: 76, Training Loss: 0.0409, Training Accuracy: 0.9886, Validation Loss: 0.6595, Validation Accuracy: 0.8560
Epoch: 77, Training Loss: 0.0320, Training Accuracy: 0.9908, Validation Loss: 0.6683, Validation Accuracy: 0.8630
Epoch: 78, Training Loss: 0.0818, Training Accuracy: 0.9800, Validation Loss: 0.6354, Validation Accuracy: 0.8635
Epoch: 79, Training Loss: 0.0302, Training Accuracy: 0.9926, Validation Loss: 0.6829, Validation Accuracy: 0.8620
Epoch: 80, Training Loss: 0.0340, Training Accuracy: 0.9908, Validation Loss: 0.7317, Validation Accuracy: 0.8535
Epoch: 81, Training Loss: 0.0445, Training Accuracy: 0.9879, Validation Loss: 0.6967, Validation Accuracy: 0.8605
Epoch: 82, Training Loss: 0.0303, Training Accuracy: 0.9921, Validation Loss: 0.7124, Validation Accuracy: 0.8575
Epoch: 83, Training Loss: 0.0798, Training Accuracy: 0.9830, Validation Loss: 0.6977, Validation Accuracy: 0.8600
Epoch: 84, Training Loss: 0.0231, Training Accuracy: 0.9951, Validation Loss: 0.7107, Validation Accuracy: 0.8600
Epoch: 85, Training Loss: 0.0211, Training Accuracy: 0.9955, Validation Loss: 0.7282, Validation Accuracy: 0.8605
Epoch: 86, Training Loss: 0.0541, Training Accuracy: 0.9861, Validation Loss: 0.7207, Validation Accuracy: 0.8585
Epoch: 87, Training Loss: 0.0208, Training Accuracy: 0.9960, Validation Loss: 0.7536, Validation Accuracy: 0.8570
Epoch: 88, Training Loss: 0.0994, Training Accuracy: 0.9798, Validation Loss: 0.6928, Validation Accuracy: 0.8650
Epoch: 89, Training Loss: 0.0197, Training Accuracy: 0.9964, Validation Loss: 0.7326, Validation Accuracy: 0.8640
Epoch: 90, Training Loss: 0.0484, Training Accuracy: 0.9868, Validation Loss: 0.7143, Validation Accuracy: 0.8590
Epoch: 91, Training Loss: 0.0177, Training Accuracy: 0.9970, Validation Loss: 0.7489, Validation Accuracy: 0.8615
Epoch: 92, Training Loss: 0.0143, Training Accuracy: 0.9981, Validation Loss: 0.7613, Validation Accuracy: 0.8630
Epoch: 93, Training Loss: 0.0722, Training Accuracy: 0.9830, Validation Loss: 0.7121, Validation Accuracy: 0.8600
Epoch: 94, Training Loss: 0.0282, Training Accuracy: 0.9930, Validation Loss: 0.7402, Validation Accuracy: 0.8630
Epoch: 95, Training Loss: 0.0137, Training Accuracy: 0.9979, Validation Loss: 0.7678, Validation Accuracy: 0.8625
Epoch: 96, Training Loss: 0.0142, Training Accuracy: 0.9978, Validation Loss: 0.8646, Validation Accuracy: 0.8525
Epoch: 97, Training Loss: 0.1122, Training Accuracy: 0.9742, Validation Loss: 0.7428, Validation Accuracy: 0.8615
Epoch: 98, Training Loss: 0.0150, Training Accuracy: 0.9976, Validation Loss: 0.7699, Validation Accuracy: 0.8590
Epoch: 99, Training Loss: 0.0177, Training Accuracy: 0.9960, Validation Loss: 0.7663, Validation Accuracy: 0.8640
Epoch: 100, Training Loss: 0.0549, Training Accuracy: 0.9861, Validation Loss: 0.7452, Validation Accuracy: 0.8600

0.85
[33]
num_params = sum(p.numel() for p in modelb.parameters())
print(f"Number of parameters in Model b: {num_params}")
Number of parameters in Model b: 235146

[78]
# Plot learning curves
import matplotlib.pyplot as plt

plt.plot(hista['eval_loss'], label='Model A')
plt.plot(histb['eval_loss'], label='Model B')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Loss')
plt.legend()
plt.show()

# Add your code

Test acc: model A 0.848, model B 0.866


[34]
# Plot learning curves
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(hista['eval_loss'], label='Model A')
plt.plot(histb['eval_loss'], label='Model B')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Loss')
plt.legend()
plt.show()
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(hista['eval_acc'], label='Model A')
plt.plot(histb['eval_acc'], label='Model B')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Accuracy')
plt.legend()
plt.show()

[<matplotlib.lines.Line2D at 0x1a76a549a60>]
[<matplotlib.lines.Line2D at 0x1a76a549e50>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Loss')
<matplotlib.legend.Legend at 0x1a76a549d00>

[<matplotlib.lines.Line2D at 0x1a76a9473a0>]
[<matplotlib.lines.Line2D at 0x1a76a947640>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Accuracy')
<matplotlib.legend.Legend at 0x1a76a93c280>

ANALYSIS
1- Model A has 50,890 parameters and Model B has 235,146 parameters.

2- An underfit model's learning curve has a high validation loss at the beginning that gradually decreases with the addition of training examples and suddenly falls to an arbitrary minimum at the end (this sudden fall at the end may not always occur, but it may stay flat), indicating that adding more training examples cannot improve the model's performance on unseen data. We see underfitting in Model A , indicating that the model is not complicated enough to fit the training data. We may see overfitting in Model B since the training loss is relatively low but the validation loss is larger, indicating that the model is memorizing the training data but not generalizing well to new data.

3- Model B outperforms Model A in terms of evaluation accuracy. This is due to Model B having more parameters and being more complex, which enables it to discover more nuanced patterns in the data. Model B, on the other hand, may be overfitting to the training data and may not perform as well on fresh, unseen data. As a result, we must employ strategies such as regularization, early halting, and cross-validation to prevent overfitting and improve the model's generalization performance.

Q2: Fight overfitting using Dropout
Model C: Take the best model you achieved from Q1, add a Dropout layer after each hidden layer. Tune the dropout rate accordingly.


Model D: Take the best model you achieved from Q1, add a BatchNormalization layer after each hidden layer.


Train these two models using the training function. Plot evaluation loss / accuracy vs. epoches from the training histories of all the three models.

Write your analysis (as markdowns or in a separate pdf file) on the following:

Are Dropout/BatchNormalization good strategies to address overfitting? Do you observe any changes to the learning curve?
How did you determine the dropout rate?
[41]
# define model with dropout

class linear_model_with_dropout(nn.Module):
    def __init__(self,input_dim,hidden_units):
        super(linear_model_with_dropout, self).__init__()
        self.classifier=nn.Sequential(
            nn.Linear(in_features=input_dim, out_features=hidden_units[0]),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=hidden_units[0], out_features=hidden_units[1]),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=hidden_units[1], out_features=hidden_units[2]),
        )
        
    def forward(self,x):
        return self.classifier(x)    
    # Add your code
[45]
# dataset

modelc = linear_model_with_dropout(input_dim = 784, hidden_units=[256,128,10])
#summary(modela, (32, 784))

optimizer = torch.optim.RMSprop(modelc.parameters(), lr = 0.0005)
acc_c, histc = train_model(modelc, train_data, val_data, test_data, device, optimizer, epochs=100)

acc_c
Epoch: 1, Training Loss: 1.0305, Training Accuracy: 0.6210, Validation Loss: 0.6769, Validation Accuracy: 0.7425
Epoch: 2, Training Loss: 0.6831, Training Accuracy: 0.7496, Validation Loss: 0.5935, Validation Accuracy: 0.7780
Epoch: 3, Training Loss: 0.5988, Training Accuracy: 0.7855, Validation Loss: 0.5681, Validation Accuracy: 0.7920
Epoch: 4, Training Loss: 0.5514, Training Accuracy: 0.8013, Validation Loss: 0.5204, Validation Accuracy: 0.8030
Epoch: 5, Training Loss: 0.5121, Training Accuracy: 0.8179, Validation Loss: 0.5100, Validation Accuracy: 0.8080
Epoch: 6, Training Loss: 0.4846, Training Accuracy: 0.8279, Validation Loss: 0.4864, Validation Accuracy: 0.8175
Epoch: 7, Training Loss: 0.4572, Training Accuracy: 0.8359, Validation Loss: 0.4896, Validation Accuracy: 0.8185
Epoch: 8, Training Loss: 0.4373, Training Accuracy: 0.8433, Validation Loss: 0.4555, Validation Accuracy: 0.8275
Epoch: 9, Training Loss: 0.4184, Training Accuracy: 0.8482, Validation Loss: 0.4538, Validation Accuracy: 0.8290
Epoch: 10, Training Loss: 0.3986, Training Accuracy: 0.8552, Validation Loss: 0.4809, Validation Accuracy: 0.8250
Epoch: 11, Training Loss: 0.3833, Training Accuracy: 0.8615, Validation Loss: 0.4310, Validation Accuracy: 0.8395
Epoch: 12, Training Loss: 0.3732, Training Accuracy: 0.8635, Validation Loss: 0.4538, Validation Accuracy: 0.8315
Epoch: 13, Training Loss: 0.3567, Training Accuracy: 0.8685, Validation Loss: 0.4513, Validation Accuracy: 0.8295
Epoch: 14, Training Loss: 0.3481, Training Accuracy: 0.8744, Validation Loss: 0.4097, Validation Accuracy: 0.8465
Epoch: 15, Training Loss: 0.3335, Training Accuracy: 0.8780, Validation Loss: 0.4254, Validation Accuracy: 0.8455
Epoch: 16, Training Loss: 0.3252, Training Accuracy: 0.8811, Validation Loss: 0.4369, Validation Accuracy: 0.8415
Epoch: 17, Training Loss: 0.3191, Training Accuracy: 0.8849, Validation Loss: 0.4063, Validation Accuracy: 0.8500
Epoch: 18, Training Loss: 0.3051, Training Accuracy: 0.8905, Validation Loss: 0.4218, Validation Accuracy: 0.8465
Epoch: 19, Training Loss: 0.2996, Training Accuracy: 0.8899, Validation Loss: 0.4153, Validation Accuracy: 0.8450
Epoch: 20, Training Loss: 0.2921, Training Accuracy: 0.8934, Validation Loss: 0.3954, Validation Accuracy: 0.8570
Epoch: 21, Training Loss: 0.2849, Training Accuracy: 0.8964, Validation Loss: 0.4051, Validation Accuracy: 0.8500
Epoch: 22, Training Loss: 0.2742, Training Accuracy: 0.9005, Validation Loss: 0.4195, Validation Accuracy: 0.8500
Epoch: 23, Training Loss: 0.2667, Training Accuracy: 0.9041, Validation Loss: 0.4233, Validation Accuracy: 0.8500
Epoch: 24, Training Loss: 0.2612, Training Accuracy: 0.9065, Validation Loss: 0.4153, Validation Accuracy: 0.8515
Epoch: 25, Training Loss: 0.2554, Training Accuracy: 0.9060, Validation Loss: 0.4326, Validation Accuracy: 0.8475
Epoch: 26, Training Loss: 0.2487, Training Accuracy: 0.9084, Validation Loss: 0.4180, Validation Accuracy: 0.8515
Epoch: 27, Training Loss: 0.2441, Training Accuracy: 0.9106, Validation Loss: 0.4064, Validation Accuracy: 0.8500
Epoch: 28, Training Loss: 0.2359, Training Accuracy: 0.9139, Validation Loss: 0.4286, Validation Accuracy: 0.8435
Epoch: 29, Training Loss: 0.2304, Training Accuracy: 0.9149, Validation Loss: 0.4158, Validation Accuracy: 0.8460
Epoch: 30, Training Loss: 0.2274, Training Accuracy: 0.9184, Validation Loss: 0.4179, Validation Accuracy: 0.8610
Epoch: 31, Training Loss: 0.2198, Training Accuracy: 0.9220, Validation Loss: 0.4079, Validation Accuracy: 0.8580
Epoch: 32, Training Loss: 0.2148, Training Accuracy: 0.9217, Validation Loss: 0.4149, Validation Accuracy: 0.8590
Epoch: 33, Training Loss: 0.2090, Training Accuracy: 0.9247, Validation Loss: 0.4474, Validation Accuracy: 0.8515
Epoch: 34, Training Loss: 0.2061, Training Accuracy: 0.9259, Validation Loss: 0.4261, Validation Accuracy: 0.8580
Epoch: 35, Training Loss: 0.2028, Training Accuracy: 0.9261, Validation Loss: 0.4071, Validation Accuracy: 0.8630
Epoch: 36, Training Loss: 0.1920, Training Accuracy: 0.9315, Validation Loss: 0.4159, Validation Accuracy: 0.8645
Epoch: 37, Training Loss: 0.1897, Training Accuracy: 0.9294, Validation Loss: 0.4477, Validation Accuracy: 0.8530
Epoch: 38, Training Loss: 0.1849, Training Accuracy: 0.9320, Validation Loss: 0.4240, Validation Accuracy: 0.8610
Epoch: 39, Training Loss: 0.1818, Training Accuracy: 0.9356, Validation Loss: 0.4719, Validation Accuracy: 0.8520
Epoch: 40, Training Loss: 0.1800, Training Accuracy: 0.9365, Validation Loss: 0.4307, Validation Accuracy: 0.8635
Epoch: 41, Training Loss: 0.1667, Training Accuracy: 0.9386, Validation Loss: 0.4332, Validation Accuracy: 0.8650
Epoch: 42, Training Loss: 0.1686, Training Accuracy: 0.9386, Validation Loss: 0.4828, Validation Accuracy: 0.8500
Epoch: 43, Training Loss: 0.1670, Training Accuracy: 0.9393, Validation Loss: 0.5229, Validation Accuracy: 0.8385
Epoch: 44, Training Loss: 0.1589, Training Accuracy: 0.9435, Validation Loss: 0.4423, Validation Accuracy: 0.8640
Epoch: 45, Training Loss: 0.1568, Training Accuracy: 0.9430, Validation Loss: 0.5148, Validation Accuracy: 0.8390
Epoch: 46, Training Loss: 0.1484, Training Accuracy: 0.9451, Validation Loss: 0.4602, Validation Accuracy: 0.8590
Epoch: 47, Training Loss: 0.1475, Training Accuracy: 0.9456, Validation Loss: 0.4618, Validation Accuracy: 0.8590
Epoch: 48, Training Loss: 0.1471, Training Accuracy: 0.9445, Validation Loss: 0.5047, Validation Accuracy: 0.8550
Epoch: 49, Training Loss: 0.1418, Training Accuracy: 0.9490, Validation Loss: 0.4608, Validation Accuracy: 0.8660
Epoch: 50, Training Loss: 0.1411, Training Accuracy: 0.9486, Validation Loss: 0.4781, Validation Accuracy: 0.8610
Epoch: 51, Training Loss: 0.1359, Training Accuracy: 0.9503, Validation Loss: 0.4886, Validation Accuracy: 0.8560
Epoch: 52, Training Loss: 0.1359, Training Accuracy: 0.9514, Validation Loss: 0.4847, Validation Accuracy: 0.8605
Epoch: 53, Training Loss: 0.1306, Training Accuracy: 0.9553, Validation Loss: 0.4721, Validation Accuracy: 0.8605
Epoch: 54, Training Loss: 0.1287, Training Accuracy: 0.9536, Validation Loss: 0.5044, Validation Accuracy: 0.8630
Epoch: 55, Training Loss: 0.1227, Training Accuracy: 0.9564, Validation Loss: 0.4937, Validation Accuracy: 0.8605
Epoch: 56, Training Loss: 0.1156, Training Accuracy: 0.9614, Validation Loss: 0.5101, Validation Accuracy: 0.8545
Epoch: 57, Training Loss: 0.1152, Training Accuracy: 0.9587, Validation Loss: 0.4876, Validation Accuracy: 0.8665
Epoch: 58, Training Loss: 0.1170, Training Accuracy: 0.9573, Validation Loss: 0.5191, Validation Accuracy: 0.8605
Epoch: 59, Training Loss: 0.1142, Training Accuracy: 0.9585, Validation Loss: 0.5051, Validation Accuracy: 0.8625
Epoch: 60, Training Loss: 0.1118, Training Accuracy: 0.9590, Validation Loss: 0.4962, Validation Accuracy: 0.8600
Epoch: 61, Training Loss: 0.1103, Training Accuracy: 0.9603, Validation Loss: 0.5214, Validation Accuracy: 0.8615
Epoch: 62, Training Loss: 0.1112, Training Accuracy: 0.9605, Validation Loss: 0.5615, Validation Accuracy: 0.8535
Epoch: 63, Training Loss: 0.1083, Training Accuracy: 0.9620, Validation Loss: 0.5350, Validation Accuracy: 0.8560
Epoch: 64, Training Loss: 0.1080, Training Accuracy: 0.9614, Validation Loss: 0.5444, Validation Accuracy: 0.8620
Epoch: 65, Training Loss: 0.1037, Training Accuracy: 0.9605, Validation Loss: 0.5197, Validation Accuracy: 0.8645
Epoch: 66, Training Loss: 0.0993, Training Accuracy: 0.9630, Validation Loss: 0.5245, Validation Accuracy: 0.8600
Epoch: 67, Training Loss: 0.0973, Training Accuracy: 0.9639, Validation Loss: 0.5364, Validation Accuracy: 0.8610
Epoch: 68, Training Loss: 0.0908, Training Accuracy: 0.9690, Validation Loss: 0.5971, Validation Accuracy: 0.8475
Epoch: 69, Training Loss: 0.1028, Training Accuracy: 0.9629, Validation Loss: 0.5238, Validation Accuracy: 0.8625
Epoch: 70, Training Loss: 0.0904, Training Accuracy: 0.9671, Validation Loss: 0.5436, Validation Accuracy: 0.8625
Epoch: 71, Training Loss: 0.0899, Training Accuracy: 0.9684, Validation Loss: 0.5556, Validation Accuracy: 0.8655
Epoch: 72, Training Loss: 0.0907, Training Accuracy: 0.9701, Validation Loss: 0.5164, Validation Accuracy: 0.8665
Epoch: 73, Training Loss: 0.0902, Training Accuracy: 0.9657, Validation Loss: 0.5443, Validation Accuracy: 0.8630
Epoch: 74, Training Loss: 0.0827, Training Accuracy: 0.9710, Validation Loss: 0.5859, Validation Accuracy: 0.8525
Epoch: 75, Training Loss: 0.0805, Training Accuracy: 0.9711, Validation Loss: 0.5823, Validation Accuracy: 0.8600
Epoch: 76, Training Loss: 0.0872, Training Accuracy: 0.9708, Validation Loss: 0.5732, Validation Accuracy: 0.8610
Epoch: 77, Training Loss: 0.0876, Training Accuracy: 0.9685, Validation Loss: 0.5625, Validation Accuracy: 0.8590
Epoch: 78, Training Loss: 0.0908, Training Accuracy: 0.9684, Validation Loss: 0.5924, Validation Accuracy: 0.8635
Epoch: 79, Training Loss: 0.0780, Training Accuracy: 0.9728, Validation Loss: 0.6091, Validation Accuracy: 0.8540
Epoch: 80, Training Loss: 0.0770, Training Accuracy: 0.9748, Validation Loss: 0.5956, Validation Accuracy: 0.8610
Epoch: 81, Training Loss: 0.0749, Training Accuracy: 0.9739, Validation Loss: 0.5957, Validation Accuracy: 0.8585
Epoch: 82, Training Loss: 0.0733, Training Accuracy: 0.9746, Validation Loss: 0.6381, Validation Accuracy: 0.8565
Epoch: 83, Training Loss: 0.0806, Training Accuracy: 0.9698, Validation Loss: 0.6136, Validation Accuracy: 0.8570
Epoch: 84, Training Loss: 0.0728, Training Accuracy: 0.9746, Validation Loss: 0.6602, Validation Accuracy: 0.8505
Epoch: 85, Training Loss: 0.0734, Training Accuracy: 0.9751, Validation Loss: 0.6256, Validation Accuracy: 0.8635
Epoch: 86, Training Loss: 0.0663, Training Accuracy: 0.9736, Validation Loss: 0.6475, Validation Accuracy: 0.8655
Epoch: 87, Training Loss: 0.0656, Training Accuracy: 0.9759, Validation Loss: 0.6423, Validation Accuracy: 0.8575
Epoch: 88, Training Loss: 0.0773, Training Accuracy: 0.9725, Validation Loss: 0.6401, Validation Accuracy: 0.8575
Epoch: 89, Training Loss: 0.0659, Training Accuracy: 0.9768, Validation Loss: 0.6412, Validation Accuracy: 0.8590
Epoch: 90, Training Loss: 0.0701, Training Accuracy: 0.9750, Validation Loss: 0.6332, Validation Accuracy: 0.8590
Epoch: 91, Training Loss: 0.0639, Training Accuracy: 0.9778, Validation Loss: 0.6171, Validation Accuracy: 0.8610
Epoch: 92, Training Loss: 0.0713, Training Accuracy: 0.9732, Validation Loss: 0.6386, Validation Accuracy: 0.8600
Epoch: 93, Training Loss: 0.0607, Training Accuracy: 0.9778, Validation Loss: 0.6992, Validation Accuracy: 0.8545
Epoch: 94, Training Loss: 0.0721, Training Accuracy: 0.9734, Validation Loss: 0.6252, Validation Accuracy: 0.8625
Epoch: 95, Training Loss: 0.0613, Training Accuracy: 0.9776, Validation Loss: 0.6479, Validation Accuracy: 0.8620
Epoch: 96, Training Loss: 0.0631, Training Accuracy: 0.9758, Validation Loss: 0.6019, Validation Accuracy: 0.8645
Epoch: 97, Training Loss: 0.0690, Training Accuracy: 0.9740, Validation Loss: 0.6483, Validation Accuracy: 0.8620
Epoch: 98, Training Loss: 0.0671, Training Accuracy: 0.9752, Validation Loss: 0.6427, Validation Accuracy: 0.8590
Epoch: 99, Training Loss: 0.0556, Training Accuracy: 0.9788, Validation Loss: 0.6299, Validation Accuracy: 0.8610
Epoch: 100, Training Loss: 0.0546, Training Accuracy: 0.9810, Validation Loss: 0.6715, Validation Accuracy: 0.8615

0.8515
[81]
# Plot learning curves

# Add your code
Test acc: model A 0.848, model B 0.866, model C 0.874


[46]
# Plot learning curves
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(hista['eval_loss'], label='Model A')
plt.plot(histb['eval_loss'], label='Model B')
plt.plot(histc['eval_loss'], label='Model B-Dropout')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Loss')
plt.legend()
plt.show()
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(hista['eval_acc'], label='Model A')
plt.plot(histb['eval_acc'], label='Model B')
plt.plot(histc['eval_acc'], label='Model B-Dropout')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Accuracy')
plt.legend()
plt.show()

[<matplotlib.lines.Line2D at 0x1a76bef3a90>]
[<matplotlib.lines.Line2D at 0x1a76bef3e50>]
[<matplotlib.lines.Line2D at 0x1a76bf06190>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Loss')
<matplotlib.legend.Legend at 0x1a76bb877c0>

[<matplotlib.lines.Line2D at 0x1a76c101d30>]
[<matplotlib.lines.Line2D at 0x1a76c101d90>]
[<matplotlib.lines.Line2D at 0x1a76c10e1c0>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Accuracy')
<matplotlib.legend.Legend at 0x1a76c0eaa30>

[71]
# Define model with BatchNormalization

class linear_model_with_batchnorm(nn.Module):
    def __init__(self,input_dim,hidden_units):
        super(linear_model_with_batchnorm, self).__init__()
        self.classifier=nn.Sequential(
            nn.Linear(in_features=input_dim, out_features=hidden_units[0]),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_units[0]),
            nn.Linear(in_features=hidden_units[0], out_features=hidden_units[1]),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_units[1]),
            nn.Linear(in_features=hidden_units[1], out_features=hidden_units[2])
        )
        
    def forward(self,x):
        return self.classifier(x)    
    
   # Add your code
[72]
modeld = linear_model_with_batchnorm(input_dim = 784, hidden_units=[256,128,10])
#summary(modela, (32, 784))

optimizer = torch.optim.RMSprop(modeld.parameters(), lr = 0.0005)
acc_d, histd = train_model(modeld, train_data, val_data, test_data, device, optimizer, epochs=100)

acc_d
Epoch: 1, Training Loss: 0.6527, Training Accuracy: 0.7695, Validation Loss: 0.7224, Validation Accuracy: 0.7535
Epoch: 2, Training Loss: 0.4319, Training Accuracy: 0.8489, Validation Loss: 0.6176, Validation Accuracy: 0.7850
Epoch: 3, Training Loss: 0.3727, Training Accuracy: 0.8658, Validation Loss: 0.6712, Validation Accuracy: 0.7765
Epoch: 4, Training Loss: 0.3220, Training Accuracy: 0.8885, Validation Loss: 0.6004, Validation Accuracy: 0.7980
Epoch: 5, Training Loss: 0.2891, Training Accuracy: 0.9005, Validation Loss: 0.5661, Validation Accuracy: 0.8080
Epoch: 6, Training Loss: 0.2575, Training Accuracy: 0.9126, Validation Loss: 0.6415, Validation Accuracy: 0.8025
Epoch: 7, Training Loss: 0.2318, Training Accuracy: 0.9200, Validation Loss: 0.5858, Validation Accuracy: 0.8095
Epoch: 8, Training Loss: 0.2142, Training Accuracy: 0.9263, Validation Loss: 0.6077, Validation Accuracy: 0.8100
Epoch: 9, Training Loss: 0.1997, Training Accuracy: 0.9315, Validation Loss: 0.5299, Validation Accuracy: 0.8215
Epoch: 10, Training Loss: 0.1775, Training Accuracy: 0.9393, Validation Loss: 0.5246, Validation Accuracy: 0.8275
Epoch: 11, Training Loss: 0.1632, Training Accuracy: 0.9437, Validation Loss: 0.4903, Validation Accuracy: 0.8435
Epoch: 12, Training Loss: 0.1451, Training Accuracy: 0.9526, Validation Loss: 0.5037, Validation Accuracy: 0.8340
Epoch: 13, Training Loss: 0.1289, Training Accuracy: 0.9576, Validation Loss: 0.5450, Validation Accuracy: 0.8300
Epoch: 14, Training Loss: 0.1341, Training Accuracy: 0.9524, Validation Loss: 0.6185, Validation Accuracy: 0.8215
Epoch: 15, Training Loss: 0.1237, Training Accuracy: 0.9575, Validation Loss: 0.5998, Validation Accuracy: 0.8210
Epoch: 16, Training Loss: 0.1163, Training Accuracy: 0.9597, Validation Loss: 0.6271, Validation Accuracy: 0.8130
Epoch: 17, Training Loss: 0.1100, Training Accuracy: 0.9636, Validation Loss: 0.6749, Validation Accuracy: 0.8155
Epoch: 18, Training Loss: 0.1133, Training Accuracy: 0.9614, Validation Loss: 0.5874, Validation Accuracy: 0.8360
Epoch: 19, Training Loss: 0.0984, Training Accuracy: 0.9677, Validation Loss: 0.6373, Validation Accuracy: 0.8290
Epoch: 20, Training Loss: 0.0872, Training Accuracy: 0.9710, Validation Loss: 0.5887, Validation Accuracy: 0.8415
Epoch: 21, Training Loss: 0.0772, Training Accuracy: 0.9749, Validation Loss: 0.6226, Validation Accuracy: 0.8365
Epoch: 22, Training Loss: 0.0852, Training Accuracy: 0.9714, Validation Loss: 0.6240, Validation Accuracy: 0.8380
Epoch: 23, Training Loss: 0.0851, Training Accuracy: 0.9688, Validation Loss: 0.7858, Validation Accuracy: 0.8050
Epoch: 24, Training Loss: 0.0693, Training Accuracy: 0.9775, Validation Loss: 0.6714, Validation Accuracy: 0.8320
Epoch: 25, Training Loss: 0.0605, Training Accuracy: 0.9836, Validation Loss: 1.0830, Validation Accuracy: 0.7825
Epoch: 26, Training Loss: 0.0739, Training Accuracy: 0.9756, Validation Loss: 0.8023, Validation Accuracy: 0.8185
Epoch: 27, Training Loss: 0.0611, Training Accuracy: 0.9824, Validation Loss: 0.7747, Validation Accuracy: 0.8270
Epoch: 28, Training Loss: 0.0768, Training Accuracy: 0.9735, Validation Loss: 0.6409, Validation Accuracy: 0.8480
Epoch: 29, Training Loss: 0.0593, Training Accuracy: 0.9814, Validation Loss: 0.6665, Validation Accuracy: 0.8500
Epoch: 30, Training Loss: 0.0539, Training Accuracy: 0.9839, Validation Loss: 0.7272, Validation Accuracy: 0.8360
Epoch: 31, Training Loss: 0.0541, Training Accuracy: 0.9829, Validation Loss: 1.0390, Validation Accuracy: 0.7970
Epoch: 32, Training Loss: 0.0577, Training Accuracy: 0.9811, Validation Loss: 0.7551, Validation Accuracy: 0.8375
Epoch: 33, Training Loss: 0.0442, Training Accuracy: 0.9865, Validation Loss: 0.8050, Validation Accuracy: 0.8370
Epoch: 34, Training Loss: 0.0470, Training Accuracy: 0.9851, Validation Loss: 0.8378, Validation Accuracy: 0.8335
Epoch: 35, Training Loss: 0.0376, Training Accuracy: 0.9896, Validation Loss: 0.7230, Validation Accuracy: 0.8395
Epoch: 36, Training Loss: 0.0297, Training Accuracy: 0.9912, Validation Loss: 0.9058, Validation Accuracy: 0.8135
Epoch: 37, Training Loss: 0.0448, Training Accuracy: 0.9852, Validation Loss: 0.7356, Validation Accuracy: 0.8485
Epoch: 38, Training Loss: 0.0339, Training Accuracy: 0.9915, Validation Loss: 0.8098, Validation Accuracy: 0.8350
Epoch: 39, Training Loss: 0.0304, Training Accuracy: 0.9916, Validation Loss: 0.8919, Validation Accuracy: 0.8320
Epoch: 40, Training Loss: 0.0559, Training Accuracy: 0.9832, Validation Loss: 0.8320, Validation Accuracy: 0.8295
Epoch: 41, Training Loss: 0.0344, Training Accuracy: 0.9890, Validation Loss: 0.8633, Validation Accuracy: 0.8285
Epoch: 42, Training Loss: 0.0260, Training Accuracy: 0.9931, Validation Loss: 0.7904, Validation Accuracy: 0.8350
Epoch: 43, Training Loss: 0.0288, Training Accuracy: 0.9924, Validation Loss: 0.8160, Validation Accuracy: 0.8465
Epoch: 44, Training Loss: 0.0238, Training Accuracy: 0.9931, Validation Loss: 0.9721, Validation Accuracy: 0.8490
Epoch: 45, Training Loss: 0.0323, Training Accuracy: 0.9904, Validation Loss: 0.7798, Validation Accuracy: 0.8430
Epoch: 46, Training Loss: 0.0275, Training Accuracy: 0.9929, Validation Loss: 0.9087, Validation Accuracy: 0.8330
Epoch: 47, Training Loss: 0.0599, Training Accuracy: 0.9794, Validation Loss: 0.9249, Validation Accuracy: 0.8270
Epoch: 48, Training Loss: 0.0290, Training Accuracy: 0.9908, Validation Loss: 0.8702, Validation Accuracy: 0.8365
Epoch: 49, Training Loss: 0.0181, Training Accuracy: 0.9956, Validation Loss: 0.9604, Validation Accuracy: 0.8295
Epoch: 50, Training Loss: 0.0229, Training Accuracy: 0.9936, Validation Loss: 1.1822, Validation Accuracy: 0.8165
Epoch: 51, Training Loss: 0.0437, Training Accuracy: 0.9854, Validation Loss: 0.9062, Validation Accuracy: 0.8360
Epoch: 52, Training Loss: 0.0380, Training Accuracy: 0.9880, Validation Loss: 0.8820, Validation Accuracy: 0.8345
Epoch: 53, Training Loss: 0.0182, Training Accuracy: 0.9951, Validation Loss: 0.9953, Validation Accuracy: 0.8135
Epoch: 54, Training Loss: 0.0151, Training Accuracy: 0.9961, Validation Loss: 0.9310, Validation Accuracy: 0.8290
Epoch: 55, Training Loss: 0.0325, Training Accuracy: 0.9895, Validation Loss: 1.1704, Validation Accuracy: 0.7995
Epoch: 56, Training Loss: 0.0256, Training Accuracy: 0.9925, Validation Loss: 0.8592, Validation Accuracy: 0.8465
Epoch: 57, Training Loss: 0.0070, Training Accuracy: 0.9990, Validation Loss: 0.8141, Validation Accuracy: 0.8520
Epoch: 58, Training Loss: 0.0043, Training Accuracy: 0.9998, Validation Loss: 0.8544, Validation Accuracy: 0.8455
Epoch: 59, Training Loss: 0.0022, Training Accuracy: 1.0000, Validation Loss: 0.8773, Validation Accuracy: 0.8495
Epoch: 60, Training Loss: 0.0023, Training Accuracy: 1.0000, Validation Loss: 0.9309, Validation Accuracy: 0.8445
Epoch: 61, Training Loss: 0.1302, Training Accuracy: 0.9600, Validation Loss: 0.9457, Validation Accuracy: 0.8285
Epoch: 62, Training Loss: 0.0301, Training Accuracy: 0.9918, Validation Loss: 0.8398, Validation Accuracy: 0.8325
Epoch: 63, Training Loss: 0.0130, Training Accuracy: 0.9975, Validation Loss: 0.8183, Validation Accuracy: 0.8405
Epoch: 64, Training Loss: 0.0123, Training Accuracy: 0.9969, Validation Loss: 0.8479, Validation Accuracy: 0.8450
Epoch: 65, Training Loss: 0.0488, Training Accuracy: 0.9836, Validation Loss: 0.9912, Validation Accuracy: 0.8275
Epoch: 66, Training Loss: 0.0203, Training Accuracy: 0.9950, Validation Loss: 0.8350, Validation Accuracy: 0.8525
Epoch: 67, Training Loss: 0.0124, Training Accuracy: 0.9970, Validation Loss: 0.9329, Validation Accuracy: 0.8355
Epoch: 68, Training Loss: 0.0086, Training Accuracy: 0.9985, Validation Loss: 0.8887, Validation Accuracy: 0.8465
Epoch: 69, Training Loss: 0.0276, Training Accuracy: 0.9926, Validation Loss: 1.3766, Validation Accuracy: 0.7900
Epoch: 70, Training Loss: 0.0392, Training Accuracy: 0.9871, Validation Loss: 0.8550, Validation Accuracy: 0.8390
Epoch: 71, Training Loss: 0.0191, Training Accuracy: 0.9955, Validation Loss: 0.8638, Validation Accuracy: 0.8430
Epoch: 72, Training Loss: 0.0193, Training Accuracy: 0.9945, Validation Loss: 0.9683, Validation Accuracy: 0.8370
Epoch: 73, Training Loss: 0.0219, Training Accuracy: 0.9930, Validation Loss: 1.0500, Validation Accuracy: 0.8215
Epoch: 74, Training Loss: 0.0334, Training Accuracy: 0.9906, Validation Loss: 0.8559, Validation Accuracy: 0.8505
Epoch: 75, Training Loss: 0.0069, Training Accuracy: 0.9989, Validation Loss: 0.9232, Validation Accuracy: 0.8425
Epoch: 76, Training Loss: 0.0145, Training Accuracy: 0.9951, Validation Loss: 0.8895, Validation Accuracy: 0.8370
Epoch: 77, Training Loss: 0.0417, Training Accuracy: 0.9861, Validation Loss: 0.8975, Validation Accuracy: 0.8415
Epoch: 78, Training Loss: 0.0196, Training Accuracy: 0.9936, Validation Loss: 0.8774, Validation Accuracy: 0.8405
Epoch: 79, Training Loss: 0.0215, Training Accuracy: 0.9940, Validation Loss: 0.8531, Validation Accuracy: 0.8515
Epoch: 80, Training Loss: 0.0049, Training Accuracy: 0.9994, Validation Loss: 0.8845, Validation Accuracy: 0.8560
Epoch: 81, Training Loss: 0.0049, Training Accuracy: 0.9991, Validation Loss: 1.0026, Validation Accuracy: 0.8355
Epoch: 82, Training Loss: 0.0550, Training Accuracy: 0.9828, Validation Loss: 1.0097, Validation Accuracy: 0.8275
Epoch: 83, Training Loss: 0.0381, Training Accuracy: 0.9882, Validation Loss: 0.8911, Validation Accuracy: 0.8315
Epoch: 84, Training Loss: 0.0166, Training Accuracy: 0.9950, Validation Loss: 0.8954, Validation Accuracy: 0.8510
Epoch: 85, Training Loss: 0.0118, Training Accuracy: 0.9970, Validation Loss: 0.9279, Validation Accuracy: 0.8485
Epoch: 86, Training Loss: 0.0069, Training Accuracy: 0.9985, Validation Loss: 0.9642, Validation Accuracy: 0.8475
Epoch: 87, Training Loss: 0.0023, Training Accuracy: 0.9999, Validation Loss: 0.9446, Validation Accuracy: 0.8555
Epoch: 88, Training Loss: 0.0012, Training Accuracy: 1.0000, Validation Loss: 0.9643, Validation Accuracy: 0.8605
Epoch: 89, Training Loss: 0.0009, Training Accuracy: 1.0000, Validation Loss: 0.9684, Validation Accuracy: 0.8575
Epoch: 90, Training Loss: 0.0006, Training Accuracy: 1.0000, Validation Loss: 0.9866, Validation Accuracy: 0.8590
Epoch: 91, Training Loss: 0.0005, Training Accuracy: 1.0000, Validation Loss: 1.0023, Validation Accuracy: 0.8585
Epoch: 92, Training Loss: 0.0004, Training Accuracy: 1.0000, Validation Loss: 1.0181, Validation Accuracy: 0.8585
Epoch: 93, Training Loss: 0.0003, Training Accuracy: 1.0000, Validation Loss: 1.0391, Validation Accuracy: 0.8585
Epoch: 94, Training Loss: 0.0003, Training Accuracy: 1.0000, Validation Loss: 1.0615, Validation Accuracy: 0.8565
Epoch: 95, Training Loss: 0.0002, Training Accuracy: 1.0000, Validation Loss: 1.0760, Validation Accuracy: 0.8570
Epoch: 96, Training Loss: 0.0966, Training Accuracy: 0.9754, Validation Loss: 2.7683, Validation Accuracy: 0.7240
Epoch: 97, Training Loss: 0.1300, Training Accuracy: 0.9563, Validation Loss: 0.8935, Validation Accuracy: 0.8390
Epoch: 98, Training Loss: 0.0301, Training Accuracy: 0.9904, Validation Loss: 0.8849, Validation Accuracy: 0.8410
Epoch: 99, Training Loss: 0.0133, Training Accuracy: 0.9971, Validation Loss: 0.8966, Validation Accuracy: 0.8430
Epoch: 100, Training Loss: 0.0115, Training Accuracy: 0.9971, Validation Loss: 0.9428, Validation Accuracy: 0.8410

0.8375
[73]
# Plot learning curves
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(hista['eval_loss'], label='Model A')
plt.plot(histb['eval_loss'], label='Model B')
plt.plot(histc['eval_loss'], label='Model B-Dropout')
plt.plot(histd['eval_loss'], label='Model B-BatchNormalization')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Loss')
plt.legend()
plt.show()
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(hista['eval_acc'], label='Model A')
plt.plot(histb['eval_acc'], label='Model B')
plt.plot(histc['eval_acc'], label='Model B-Dropout')
plt.plot(histd['eval_acc'], label='Model B-BatchNormalization')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Accuracy')
plt.legend()
plt.show()
[<matplotlib.lines.Line2D at 0x1a770a8f160>]
[<matplotlib.lines.Line2D at 0x1a770a8f400>]
[<matplotlib.lines.Line2D at 0x1a770a8f5e0>]
[<matplotlib.lines.Line2D at 0x1a770a8f970>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Loss')
<matplotlib.legend.Legend at 0x1a76df6a6a0>

[<matplotlib.lines.Line2D at 0x1a770ae9700>]
[<matplotlib.lines.Line2D at 0x1a770ae99a0>]
[<matplotlib.lines.Line2D at 0x1a770ae9c70>]
[<matplotlib.lines.Line2D at 0x1a770ae9f40>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Accuracy')
<matplotlib.legend.Legend at 0x1a770ae9df0>

[84]
# Plot learning curves

  # Add your code
Test acc: model A 0.848, model B 0.866, model C 0.874, model D 0.876


Analysis
As such, Dropout and Batch Normalization are popular solutions for dealing with overfitting in neural networks. Dropout randomly removes (i.e., sets to zero) portion of a layer's input units during training to minimize overfitting. This keeps the network from being overly reliant on any single feature or input combination, forcing it to learn more robust representations. Batch Normalization, on the other hand, normalizes each layer's activations, making the network more resistant to internal covariate shift and allowing it to acquire more generalizable features. In general, adding Dropout or Batch Normalization layers to a model can improve generalization performance by reducing overfitting. Their success, however, is dependent on the unique situation and model architecture. In my particular model, introducing Dropout boosted validation accuracy while decreasing loss; however, with batch normalization, validation accuracy decreased while loss climbed significantly. As a result, we will solely employ dropout for my model.

2-The dropout rate can be determined through cross-validation or by evaluating the model's performance on a validation set. A popular method is to begin with a low dropout rate, such as 0.1 or 0.2, and progressively increase it until overfitting is decreased.I started with 0.1 and worked my way up to 0.7 for dropout ratio, however I discovered that my model performed best with smaller dropout ratios.

Q3: Optimizer
For Q1 and Q2, we use RMSprop (or any other optimizer you prefer). Now let's train the best model you get in Q1 using the following optimizers (for details, check https://pytorch.org/docs/stable/optim.html?highlight=optimizer#torch.optim.Optimizer):
SGD
Adagrad
RMSProp
Adam
Plot the evaluation loss and accuracy curves for each optimizer.

Write your analysis (as markdowns) on the following:
Which optimizer works the best? What makes this optimizer more effective?
Which has the worst performance? Why does it perform poorly?
[52]
modelsgd = linear_model_B(input_dim = 784, hidden_units=[256,128,10])
#summary(modela, (32, 784))

optimizer = torch.optim.SGD(modelsgd.parameters(), lr = 0.0005)
acc_sgd, histsgd = train_model(modelsgd, train_data, val_data, test_data, device, optimizer, epochs=100)

acc_sgd
Epoch: 1, Training Loss: 2.2963, Training Accuracy: 0.0917, Validation Loss: 2.2959, Validation Accuracy: 0.0890
Epoch: 2, Training Loss: 2.2932, Training Accuracy: 0.1037, Validation Loss: 2.2928, Validation Accuracy: 0.0980
Epoch: 3, Training Loss: 2.2901, Training Accuracy: 0.1219, Validation Loss: 2.2897, Validation Accuracy: 0.1245
Epoch: 4, Training Loss: 2.2871, Training Accuracy: 0.1460, Validation Loss: 2.2865, Validation Accuracy: 0.1565
Epoch: 5, Training Loss: 2.2840, Training Accuracy: 0.1824, Validation Loss: 2.2834, Validation Accuracy: 0.1870
Epoch: 6, Training Loss: 2.2809, Training Accuracy: 0.2067, Validation Loss: 2.2802, Validation Accuracy: 0.2090
Epoch: 7, Training Loss: 2.2778, Training Accuracy: 0.2239, Validation Loss: 2.2771, Validation Accuracy: 0.2200
Epoch: 8, Training Loss: 2.2748, Training Accuracy: 0.2303, Validation Loss: 2.2741, Validation Accuracy: 0.2250
Epoch: 9, Training Loss: 2.2718, Training Accuracy: 0.2359, Validation Loss: 2.2711, Validation Accuracy: 0.2275
Epoch: 10, Training Loss: 2.2689, Training Accuracy: 0.2362, Validation Loss: 2.2681, Validation Accuracy: 0.2295
Epoch: 11, Training Loss: 2.2660, Training Accuracy: 0.2366, Validation Loss: 2.2652, Validation Accuracy: 0.2295
Epoch: 12, Training Loss: 2.2632, Training Accuracy: 0.2352, Validation Loss: 2.2623, Validation Accuracy: 0.2310
Epoch: 13, Training Loss: 2.2603, Training Accuracy: 0.2352, Validation Loss: 2.2594, Validation Accuracy: 0.2300
Epoch: 14, Training Loss: 2.2575, Training Accuracy: 0.2343, Validation Loss: 2.2565, Validation Accuracy: 0.2300
Epoch: 15, Training Loss: 2.2547, Training Accuracy: 0.2335, Validation Loss: 2.2536, Validation Accuracy: 0.2290
Epoch: 16, Training Loss: 2.2518, Training Accuracy: 0.2331, Validation Loss: 2.2506, Validation Accuracy: 0.2280
Epoch: 17, Training Loss: 2.2490, Training Accuracy: 0.2323, Validation Loss: 2.2477, Validation Accuracy: 0.2270
Epoch: 18, Training Loss: 2.2461, Training Accuracy: 0.2313, Validation Loss: 2.2447, Validation Accuracy: 0.2275
Epoch: 19, Training Loss: 2.2431, Training Accuracy: 0.2320, Validation Loss: 2.2417, Validation Accuracy: 0.2270
Epoch: 20, Training Loss: 2.2402, Training Accuracy: 0.2318, Validation Loss: 2.2387, Validation Accuracy: 0.2275
Epoch: 21, Training Loss: 2.2372, Training Accuracy: 0.2320, Validation Loss: 2.2356, Validation Accuracy: 0.2290
Epoch: 22, Training Loss: 2.2342, Training Accuracy: 0.2334, Validation Loss: 2.2325, Validation Accuracy: 0.2310
Epoch: 23, Training Loss: 2.2311, Training Accuracy: 0.2361, Validation Loss: 2.2294, Validation Accuracy: 0.2325
Epoch: 24, Training Loss: 2.2280, Training Accuracy: 0.2381, Validation Loss: 2.2262, Validation Accuracy: 0.2355
Epoch: 25, Training Loss: 2.2248, Training Accuracy: 0.2417, Validation Loss: 2.2230, Validation Accuracy: 0.2400
Epoch: 26, Training Loss: 2.2216, Training Accuracy: 0.2456, Validation Loss: 2.2197, Validation Accuracy: 0.2425
Epoch: 27, Training Loss: 2.2183, Training Accuracy: 0.2482, Validation Loss: 2.2163, Validation Accuracy: 0.2505
Epoch: 28, Training Loss: 2.2150, Training Accuracy: 0.2507, Validation Loss: 2.2129, Validation Accuracy: 0.2545
Epoch: 29, Training Loss: 2.2116, Training Accuracy: 0.2559, Validation Loss: 2.2094, Validation Accuracy: 0.2605
Epoch: 30, Training Loss: 2.2082, Training Accuracy: 0.2585, Validation Loss: 2.2059, Validation Accuracy: 0.2630
Epoch: 31, Training Loss: 2.2047, Training Accuracy: 0.2629, Validation Loss: 2.2023, Validation Accuracy: 0.2650
Epoch: 32, Training Loss: 2.2011, Training Accuracy: 0.2670, Validation Loss: 2.1986, Validation Accuracy: 0.2670
Epoch: 33, Training Loss: 2.1974, Training Accuracy: 0.2692, Validation Loss: 2.1948, Validation Accuracy: 0.2710
Epoch: 34, Training Loss: 2.1937, Training Accuracy: 0.2717, Validation Loss: 2.1910, Validation Accuracy: 0.2745
Epoch: 35, Training Loss: 2.1899, Training Accuracy: 0.2751, Validation Loss: 2.1870, Validation Accuracy: 0.2795
Epoch: 36, Training Loss: 2.1859, Training Accuracy: 0.2784, Validation Loss: 2.1830, Validation Accuracy: 0.2835
Epoch: 37, Training Loss: 2.1819, Training Accuracy: 0.2823, Validation Loss: 2.1789, Validation Accuracy: 0.2895
Epoch: 38, Training Loss: 2.1778, Training Accuracy: 0.2851, Validation Loss: 2.1746, Validation Accuracy: 0.2920
Epoch: 39, Training Loss: 2.1736, Training Accuracy: 0.2876, Validation Loss: 2.1703, Validation Accuracy: 0.2935
Epoch: 40, Training Loss: 2.1694, Training Accuracy: 0.2900, Validation Loss: 2.1659, Validation Accuracy: 0.2940
Epoch: 41, Training Loss: 2.1650, Training Accuracy: 0.2924, Validation Loss: 2.1614, Validation Accuracy: 0.2980
Epoch: 42, Training Loss: 2.1604, Training Accuracy: 0.2954, Validation Loss: 2.1567, Validation Accuracy: 0.3000
Epoch: 43, Training Loss: 2.1558, Training Accuracy: 0.2985, Validation Loss: 2.1519, Validation Accuracy: 0.3060
Epoch: 44, Training Loss: 2.1511, Training Accuracy: 0.3029, Validation Loss: 2.1471, Validation Accuracy: 0.3100
Epoch: 45, Training Loss: 2.1463, Training Accuracy: 0.3064, Validation Loss: 2.1421, Validation Accuracy: 0.3125
Epoch: 46, Training Loss: 2.1413, Training Accuracy: 0.3096, Validation Loss: 2.1369, Validation Accuracy: 0.3165
Epoch: 47, Training Loss: 2.1363, Training Accuracy: 0.3135, Validation Loss: 2.1317, Validation Accuracy: 0.3190
Epoch: 48, Training Loss: 2.1311, Training Accuracy: 0.3166, Validation Loss: 2.1264, Validation Accuracy: 0.3225
Epoch: 49, Training Loss: 2.1258, Training Accuracy: 0.3190, Validation Loss: 2.1209, Validation Accuracy: 0.3235
Epoch: 50, Training Loss: 2.1203, Training Accuracy: 0.3217, Validation Loss: 2.1153, Validation Accuracy: 0.3260
Epoch: 51, Training Loss: 2.1148, Training Accuracy: 0.3255, Validation Loss: 2.1095, Validation Accuracy: 0.3300
Epoch: 52, Training Loss: 2.1091, Training Accuracy: 0.3284, Validation Loss: 2.1036, Validation Accuracy: 0.3335
Epoch: 53, Training Loss: 2.1033, Training Accuracy: 0.3304, Validation Loss: 2.0976, Validation Accuracy: 0.3370
Epoch: 54, Training Loss: 2.0974, Training Accuracy: 0.3325, Validation Loss: 2.0915, Validation Accuracy: 0.3395
Epoch: 55, Training Loss: 2.0913, Training Accuracy: 0.3355, Validation Loss: 2.0852, Validation Accuracy: 0.3435
Epoch: 56, Training Loss: 2.0851, Training Accuracy: 0.3377, Validation Loss: 2.0788, Validation Accuracy: 0.3440
Epoch: 57, Training Loss: 2.0788, Training Accuracy: 0.3410, Validation Loss: 2.0723, Validation Accuracy: 0.3490
Epoch: 58, Training Loss: 2.0723, Training Accuracy: 0.3463, Validation Loss: 2.0655, Validation Accuracy: 0.3515
Epoch: 59, Training Loss: 2.0657, Training Accuracy: 0.3499, Validation Loss: 2.0587, Validation Accuracy: 0.3560
Epoch: 60, Training Loss: 2.0590, Training Accuracy: 0.3559, Validation Loss: 2.0517, Validation Accuracy: 0.3610
Epoch: 61, Training Loss: 2.0521, Training Accuracy: 0.3635, Validation Loss: 2.0445, Validation Accuracy: 0.3680
Epoch: 62, Training Loss: 2.0450, Training Accuracy: 0.3694, Validation Loss: 2.0372, Validation Accuracy: 0.3735
Epoch: 63, Training Loss: 2.0378, Training Accuracy: 0.3767, Validation Loss: 2.0297, Validation Accuracy: 0.3830
Epoch: 64, Training Loss: 2.0304, Training Accuracy: 0.3857, Validation Loss: 2.0221, Validation Accuracy: 0.3930
Epoch: 65, Training Loss: 2.0229, Training Accuracy: 0.3936, Validation Loss: 2.0143, Validation Accuracy: 0.4000
Epoch: 66, Training Loss: 2.0153, Training Accuracy: 0.4026, Validation Loss: 2.0063, Validation Accuracy: 0.4105
Epoch: 67, Training Loss: 2.0075, Training Accuracy: 0.4088, Validation Loss: 1.9982, Validation Accuracy: 0.4220
Epoch: 68, Training Loss: 1.9995, Training Accuracy: 0.4134, Validation Loss: 1.9899, Validation Accuracy: 0.4285
Epoch: 69, Training Loss: 1.9914, Training Accuracy: 0.4196, Validation Loss: 1.9815, Validation Accuracy: 0.4335
Epoch: 70, Training Loss: 1.9831, Training Accuracy: 0.4273, Validation Loss: 1.9729, Validation Accuracy: 0.4425
Epoch: 71, Training Loss: 1.9746, Training Accuracy: 0.4348, Validation Loss: 1.9641, Validation Accuracy: 0.4465
Epoch: 72, Training Loss: 1.9660, Training Accuracy: 0.4419, Validation Loss: 1.9552, Validation Accuracy: 0.4505
Epoch: 73, Training Loss: 1.9573, Training Accuracy: 0.4486, Validation Loss: 1.9461, Validation Accuracy: 0.4625
Epoch: 74, Training Loss: 1.9484, Training Accuracy: 0.4552, Validation Loss: 1.9369, Validation Accuracy: 0.4715
Epoch: 75, Training Loss: 1.9393, Training Accuracy: 0.4614, Validation Loss: 1.9275, Validation Accuracy: 0.4780
Epoch: 76, Training Loss: 1.9301, Training Accuracy: 0.4698, Validation Loss: 1.9180, Validation Accuracy: 0.4865
Epoch: 77, Training Loss: 1.9207, Training Accuracy: 0.4754, Validation Loss: 1.9083, Validation Accuracy: 0.4895
Epoch: 78, Training Loss: 1.9112, Training Accuracy: 0.4814, Validation Loss: 1.8984, Validation Accuracy: 0.4960
Epoch: 79, Training Loss: 1.9016, Training Accuracy: 0.4865, Validation Loss: 1.8885, Validation Accuracy: 0.5030
Epoch: 80, Training Loss: 1.8918, Training Accuracy: 0.4915, Validation Loss: 1.8784, Validation Accuracy: 0.5085
Epoch: 81, Training Loss: 1.8819, Training Accuracy: 0.4968, Validation Loss: 1.8681, Validation Accuracy: 0.5135
Epoch: 82, Training Loss: 1.8719, Training Accuracy: 0.5024, Validation Loss: 1.8578, Validation Accuracy: 0.5190
Epoch: 83, Training Loss: 1.8618, Training Accuracy: 0.5060, Validation Loss: 1.8473, Validation Accuracy: 0.5240
Epoch: 84, Training Loss: 1.8515, Training Accuracy: 0.5099, Validation Loss: 1.8368, Validation Accuracy: 0.5225
Epoch: 85, Training Loss: 1.8412, Training Accuracy: 0.5145, Validation Loss: 1.8261, Validation Accuracy: 0.5280
Epoch: 86, Training Loss: 1.8307, Training Accuracy: 0.5172, Validation Loss: 1.8153, Validation Accuracy: 0.5325
Epoch: 87, Training Loss: 1.8202, Training Accuracy: 0.5206, Validation Loss: 1.8045, Validation Accuracy: 0.5325
Epoch: 88, Training Loss: 1.8096, Training Accuracy: 0.5236, Validation Loss: 1.7936, Validation Accuracy: 0.5340
Epoch: 89, Training Loss: 1.7990, Training Accuracy: 0.5271, Validation Loss: 1.7826, Validation Accuracy: 0.5365
Epoch: 90, Training Loss: 1.7883, Training Accuracy: 0.5305, Validation Loss: 1.7716, Validation Accuracy: 0.5400
Epoch: 91, Training Loss: 1.7775, Training Accuracy: 0.5336, Validation Loss: 1.7605, Validation Accuracy: 0.5405
Epoch: 92, Training Loss: 1.7667, Training Accuracy: 0.5363, Validation Loss: 1.7495, Validation Accuracy: 0.5405
Epoch: 93, Training Loss: 1.7558, Training Accuracy: 0.5387, Validation Loss: 1.7383, Validation Accuracy: 0.5430
Epoch: 94, Training Loss: 1.7450, Training Accuracy: 0.5419, Validation Loss: 1.7272, Validation Accuracy: 0.5465
Epoch: 95, Training Loss: 1.7341, Training Accuracy: 0.5444, Validation Loss: 1.7161, Validation Accuracy: 0.5520
Epoch: 96, Training Loss: 1.7232, Training Accuracy: 0.5471, Validation Loss: 1.7050, Validation Accuracy: 0.5550
Epoch: 97, Training Loss: 1.7124, Training Accuracy: 0.5493, Validation Loss: 1.6939, Validation Accuracy: 0.5580
Epoch: 98, Training Loss: 1.7016, Training Accuracy: 0.5523, Validation Loss: 1.6828, Validation Accuracy: 0.5585
Epoch: 99, Training Loss: 1.6907, Training Accuracy: 0.5551, Validation Loss: 1.6718, Validation Accuracy: 0.5600
Epoch: 100, Training Loss: 1.6800, Training Accuracy: 0.5590, Validation Loss: 1.6608, Validation Accuracy: 0.5620

0.5495
[54]
# Train model using AdaGrad
modeladagrad = linear_model_B(input_dim = 784, hidden_units=[256,128,10])
#summary(modela, (32, 784))

optimizer = torch.optim.Adagrad(modeladagrad.parameters(), lr = 0.0005)
acc_adagrad, histadagrad = train_model(modeladagrad, train_data, val_data, test_data, device, optimizer, epochs=100)

acc_adagrad
Epoch: 1, Training Loss: 1.8127, Training Accuracy: 0.5593, Validation Loss: 1.4423, Validation Accuracy: 0.6510
Epoch: 2, Training Loss: 1.3000, Training Accuracy: 0.6535, Validation Loss: 1.1402, Validation Accuracy: 0.6685
Epoch: 3, Training Loss: 1.0928, Training Accuracy: 0.6625, Validation Loss: 0.9988, Validation Accuracy: 0.6815
Epoch: 4, Training Loss: 0.9852, Training Accuracy: 0.6747, Validation Loss: 0.9175, Validation Accuracy: 0.6985
Epoch: 5, Training Loss: 0.9183, Training Accuracy: 0.6863, Validation Loss: 0.8630, Validation Accuracy: 0.7065
Epoch: 6, Training Loss: 0.8713, Training Accuracy: 0.6983, Validation Loss: 0.8239, Validation Accuracy: 0.7180
Epoch: 7, Training Loss: 0.8366, Training Accuracy: 0.7069, Validation Loss: 0.7943, Validation Accuracy: 0.7260
Epoch: 8, Training Loss: 0.8093, Training Accuracy: 0.7163, Validation Loss: 0.7706, Validation Accuracy: 0.7355
Epoch: 9, Training Loss: 0.7870, Training Accuracy: 0.7236, Validation Loss: 0.7512, Validation Accuracy: 0.7400
Epoch: 10, Training Loss: 0.7683, Training Accuracy: 0.7310, Validation Loss: 0.7346, Validation Accuracy: 0.7455
Epoch: 11, Training Loss: 0.7522, Training Accuracy: 0.7384, Validation Loss: 0.7205, Validation Accuracy: 0.7515
Epoch: 12, Training Loss: 0.7383, Training Accuracy: 0.7421, Validation Loss: 0.7083, Validation Accuracy: 0.7540
Epoch: 13, Training Loss: 0.7260, Training Accuracy: 0.7451, Validation Loss: 0.6975, Validation Accuracy: 0.7560
Epoch: 14, Training Loss: 0.7150, Training Accuracy: 0.7499, Validation Loss: 0.6879, Validation Accuracy: 0.7600
Epoch: 15, Training Loss: 0.7050, Training Accuracy: 0.7529, Validation Loss: 0.6791, Validation Accuracy: 0.7640
Epoch: 16, Training Loss: 0.6959, Training Accuracy: 0.7570, Validation Loss: 0.6711, Validation Accuracy: 0.7665
Epoch: 17, Training Loss: 0.6874, Training Accuracy: 0.7592, Validation Loss: 0.6638, Validation Accuracy: 0.7685
Epoch: 18, Training Loss: 0.6795, Training Accuracy: 0.7626, Validation Loss: 0.6570, Validation Accuracy: 0.7710
Epoch: 19, Training Loss: 0.6723, Training Accuracy: 0.7662, Validation Loss: 0.6508, Validation Accuracy: 0.7750
Epoch: 20, Training Loss: 0.6655, Training Accuracy: 0.7688, Validation Loss: 0.6450, Validation Accuracy: 0.7770
Epoch: 21, Training Loss: 0.6592, Training Accuracy: 0.7714, Validation Loss: 0.6396, Validation Accuracy: 0.7805
Epoch: 22, Training Loss: 0.6532, Training Accuracy: 0.7739, Validation Loss: 0.6346, Validation Accuracy: 0.7840
Epoch: 23, Training Loss: 0.6476, Training Accuracy: 0.7759, Validation Loss: 0.6299, Validation Accuracy: 0.7860
Epoch: 24, Training Loss: 0.6423, Training Accuracy: 0.7780, Validation Loss: 0.6254, Validation Accuracy: 0.7885
Epoch: 25, Training Loss: 0.6372, Training Accuracy: 0.7806, Validation Loss: 0.6212, Validation Accuracy: 0.7900
Epoch: 26, Training Loss: 0.6324, Training Accuracy: 0.7823, Validation Loss: 0.6172, Validation Accuracy: 0.7920
Epoch: 27, Training Loss: 0.6278, Training Accuracy: 0.7849, Validation Loss: 0.6134, Validation Accuracy: 0.7935
Epoch: 28, Training Loss: 0.6234, Training Accuracy: 0.7864, Validation Loss: 0.6098, Validation Accuracy: 0.7940
Epoch: 29, Training Loss: 0.6193, Training Accuracy: 0.7879, Validation Loss: 0.6064, Validation Accuracy: 0.7945
Epoch: 30, Training Loss: 0.6152, Training Accuracy: 0.7895, Validation Loss: 0.6030, Validation Accuracy: 0.7955
Epoch: 31, Training Loss: 0.6113, Training Accuracy: 0.7911, Validation Loss: 0.5998, Validation Accuracy: 0.7970
Epoch: 32, Training Loss: 0.6075, Training Accuracy: 0.7923, Validation Loss: 0.5968, Validation Accuracy: 0.7975
Epoch: 33, Training Loss: 0.6039, Training Accuracy: 0.7943, Validation Loss: 0.5939, Validation Accuracy: 0.7985
Epoch: 34, Training Loss: 0.6004, Training Accuracy: 0.7949, Validation Loss: 0.5911, Validation Accuracy: 0.8005
Epoch: 35, Training Loss: 0.5970, Training Accuracy: 0.7969, Validation Loss: 0.5884, Validation Accuracy: 0.8015
Epoch: 36, Training Loss: 0.5938, Training Accuracy: 0.7976, Validation Loss: 0.5859, Validation Accuracy: 0.8015
Epoch: 37, Training Loss: 0.5907, Training Accuracy: 0.7991, Validation Loss: 0.5834, Validation Accuracy: 0.8020
Epoch: 38, Training Loss: 0.5877, Training Accuracy: 0.8004, Validation Loss: 0.5810, Validation Accuracy: 0.8035
Epoch: 39, Training Loss: 0.5847, Training Accuracy: 0.8017, Validation Loss: 0.5788, Validation Accuracy: 0.8045
Epoch: 40, Training Loss: 0.5819, Training Accuracy: 0.8033, Validation Loss: 0.5766, Validation Accuracy: 0.8045
Epoch: 41, Training Loss: 0.5792, Training Accuracy: 0.8051, Validation Loss: 0.5744, Validation Accuracy: 0.8050
Epoch: 42, Training Loss: 0.5766, Training Accuracy: 0.8059, Validation Loss: 0.5724, Validation Accuracy: 0.8060
Epoch: 43, Training Loss: 0.5740, Training Accuracy: 0.8070, Validation Loss: 0.5704, Validation Accuracy: 0.8055
Epoch: 44, Training Loss: 0.5715, Training Accuracy: 0.8093, Validation Loss: 0.5685, Validation Accuracy: 0.8065
Epoch: 45, Training Loss: 0.5691, Training Accuracy: 0.8094, Validation Loss: 0.5667, Validation Accuracy: 0.8065
Epoch: 46, Training Loss: 0.5667, Training Accuracy: 0.8104, Validation Loss: 0.5649, Validation Accuracy: 0.8070
Epoch: 47, Training Loss: 0.5645, Training Accuracy: 0.8111, Validation Loss: 0.5632, Validation Accuracy: 0.8075
Epoch: 48, Training Loss: 0.5622, Training Accuracy: 0.8115, Validation Loss: 0.5615, Validation Accuracy: 0.8070
Epoch: 49, Training Loss: 0.5601, Training Accuracy: 0.8121, Validation Loss: 0.5599, Validation Accuracy: 0.8080
Epoch: 50, Training Loss: 0.5580, Training Accuracy: 0.8126, Validation Loss: 0.5583, Validation Accuracy: 0.8085
Epoch: 51, Training Loss: 0.5559, Training Accuracy: 0.8134, Validation Loss: 0.5568, Validation Accuracy: 0.8085
Epoch: 52, Training Loss: 0.5540, Training Accuracy: 0.8136, Validation Loss: 0.5553, Validation Accuracy: 0.8085
Epoch: 53, Training Loss: 0.5520, Training Accuracy: 0.8143, Validation Loss: 0.5539, Validation Accuracy: 0.8090
Epoch: 54, Training Loss: 0.5501, Training Accuracy: 0.8147, Validation Loss: 0.5524, Validation Accuracy: 0.8095
Epoch: 55, Training Loss: 0.5483, Training Accuracy: 0.8153, Validation Loss: 0.5511, Validation Accuracy: 0.8100
Epoch: 56, Training Loss: 0.5465, Training Accuracy: 0.8163, Validation Loss: 0.5498, Validation Accuracy: 0.8105
Epoch: 57, Training Loss: 0.5447, Training Accuracy: 0.8175, Validation Loss: 0.5485, Validation Accuracy: 0.8110
Epoch: 58, Training Loss: 0.5430, Training Accuracy: 0.8177, Validation Loss: 0.5472, Validation Accuracy: 0.8120
Epoch: 59, Training Loss: 0.5413, Training Accuracy: 0.8185, Validation Loss: 0.5460, Validation Accuracy: 0.8120
Epoch: 60, Training Loss: 0.5396, Training Accuracy: 0.8187, Validation Loss: 0.5448, Validation Accuracy: 0.8125
Epoch: 61, Training Loss: 0.5380, Training Accuracy: 0.8194, Validation Loss: 0.5436, Validation Accuracy: 0.8130
Epoch: 62, Training Loss: 0.5364, Training Accuracy: 0.8196, Validation Loss: 0.5425, Validation Accuracy: 0.8125
Epoch: 63, Training Loss: 0.5349, Training Accuracy: 0.8196, Validation Loss: 0.5414, Validation Accuracy: 0.8130
Epoch: 64, Training Loss: 0.5334, Training Accuracy: 0.8203, Validation Loss: 0.5403, Validation Accuracy: 0.8135
Epoch: 65, Training Loss: 0.5319, Training Accuracy: 0.8203, Validation Loss: 0.5392, Validation Accuracy: 0.8140
Epoch: 66, Training Loss: 0.5304, Training Accuracy: 0.8201, Validation Loss: 0.5382, Validation Accuracy: 0.8140
Epoch: 67, Training Loss: 0.5290, Training Accuracy: 0.8209, Validation Loss: 0.5372, Validation Accuracy: 0.8145
Epoch: 68, Training Loss: 0.5276, Training Accuracy: 0.8210, Validation Loss: 0.5362, Validation Accuracy: 0.8150
Epoch: 69, Training Loss: 0.5262, Training Accuracy: 0.8213, Validation Loss: 0.5352, Validation Accuracy: 0.8150
Epoch: 70, Training Loss: 0.5249, Training Accuracy: 0.8217, Validation Loss: 0.5343, Validation Accuracy: 0.8145
Epoch: 71, Training Loss: 0.5235, Training Accuracy: 0.8220, Validation Loss: 0.5333, Validation Accuracy: 0.8150
Epoch: 72, Training Loss: 0.5222, Training Accuracy: 0.8223, Validation Loss: 0.5324, Validation Accuracy: 0.8150
Epoch: 73, Training Loss: 0.5209, Training Accuracy: 0.8231, Validation Loss: 0.5315, Validation Accuracy: 0.8160
Epoch: 74, Training Loss: 0.5197, Training Accuracy: 0.8239, Validation Loss: 0.5306, Validation Accuracy: 0.8170
Epoch: 75, Training Loss: 0.5184, Training Accuracy: 0.8244, Validation Loss: 0.5298, Validation Accuracy: 0.8175
Epoch: 76, Training Loss: 0.5172, Training Accuracy: 0.8249, Validation Loss: 0.5289, Validation Accuracy: 0.8185
Epoch: 77, Training Loss: 0.5160, Training Accuracy: 0.8254, Validation Loss: 0.5281, Validation Accuracy: 0.8190
Epoch: 78, Training Loss: 0.5148, Training Accuracy: 0.8255, Validation Loss: 0.5273, Validation Accuracy: 0.8190
Epoch: 79, Training Loss: 0.5137, Training Accuracy: 0.8264, Validation Loss: 0.5265, Validation Accuracy: 0.8190
Epoch: 80, Training Loss: 0.5125, Training Accuracy: 0.8269, Validation Loss: 0.5257, Validation Accuracy: 0.8190
Epoch: 81, Training Loss: 0.5114, Training Accuracy: 0.8273, Validation Loss: 0.5249, Validation Accuracy: 0.8200
Epoch: 82, Training Loss: 0.5103, Training Accuracy: 0.8280, Validation Loss: 0.5242, Validation Accuracy: 0.8200
Epoch: 83, Training Loss: 0.5092, Training Accuracy: 0.8284, Validation Loss: 0.5235, Validation Accuracy: 0.8200
Epoch: 84, Training Loss: 0.5081, Training Accuracy: 0.8289, Validation Loss: 0.5227, Validation Accuracy: 0.8205
Epoch: 85, Training Loss: 0.5071, Training Accuracy: 0.8295, Validation Loss: 0.5220, Validation Accuracy: 0.8210
Epoch: 86, Training Loss: 0.5061, Training Accuracy: 0.8299, Validation Loss: 0.5213, Validation Accuracy: 0.8210
Epoch: 87, Training Loss: 0.5050, Training Accuracy: 0.8299, Validation Loss: 0.5206, Validation Accuracy: 0.8205
Epoch: 88, Training Loss: 0.5040, Training Accuracy: 0.8300, Validation Loss: 0.5199, Validation Accuracy: 0.8205
Epoch: 89, Training Loss: 0.5030, Training Accuracy: 0.8304, Validation Loss: 0.5193, Validation Accuracy: 0.8215
Epoch: 90, Training Loss: 0.5020, Training Accuracy: 0.8306, Validation Loss: 0.5186, Validation Accuracy: 0.8225
Epoch: 91, Training Loss: 0.5011, Training Accuracy: 0.8309, Validation Loss: 0.5180, Validation Accuracy: 0.8230
Epoch: 92, Training Loss: 0.5001, Training Accuracy: 0.8310, Validation Loss: 0.5174, Validation Accuracy: 0.8230
Epoch: 93, Training Loss: 0.4992, Training Accuracy: 0.8313, Validation Loss: 0.5167, Validation Accuracy: 0.8230
Epoch: 94, Training Loss: 0.4982, Training Accuracy: 0.8317, Validation Loss: 0.5161, Validation Accuracy: 0.8230
Epoch: 95, Training Loss: 0.4973, Training Accuracy: 0.8317, Validation Loss: 0.5155, Validation Accuracy: 0.8230
Epoch: 96, Training Loss: 0.4964, Training Accuracy: 0.8317, Validation Loss: 0.5149, Validation Accuracy: 0.8230
Epoch: 97, Training Loss: 0.4955, Training Accuracy: 0.8320, Validation Loss: 0.5143, Validation Accuracy: 0.8230
Epoch: 98, Training Loss: 0.4946, Training Accuracy: 0.8323, Validation Loss: 0.5138, Validation Accuracy: 0.8230
Epoch: 99, Training Loss: 0.4937, Training Accuracy: 0.8325, Validation Loss: 0.5132, Validation Accuracy: 0.8240
Epoch: 100, Training Loss: 0.4929, Training Accuracy: 0.8326, Validation Loss: 0.5126, Validation Accuracy: 0.8240

0.8185
[56]
# Train model using Adam

modeladam = linear_model_B(input_dim = 784, hidden_units=[256,128,10])
#summary(modela, (32, 784))

optimizer = torch.optim.Adam(modeladam.parameters(), lr = 0.0005)
acc_adam, histadam = train_model(modeladam, train_data, val_data, test_data, device, optimizer, epochs=100)

acc_adam
Epoch: 1, Training Loss: 1.3146, Training Accuracy: 0.5879, Validation Loss: 0.7711, Validation Accuracy: 0.7315
Epoch: 2, Training Loss: 0.6854, Training Accuracy: 0.7550, Validation Loss: 0.6270, Validation Accuracy: 0.7820
Epoch: 3, Training Loss: 0.5766, Training Accuracy: 0.7959, Validation Loss: 0.5673, Validation Accuracy: 0.8015
Epoch: 4, Training Loss: 0.5205, Training Accuracy: 0.8181, Validation Loss: 0.5395, Validation Accuracy: 0.8090
Epoch: 5, Training Loss: 0.4847, Training Accuracy: 0.8301, Validation Loss: 0.5221, Validation Accuracy: 0.8175
Epoch: 6, Training Loss: 0.4581, Training Accuracy: 0.8397, Validation Loss: 0.5046, Validation Accuracy: 0.8200
Epoch: 7, Training Loss: 0.4350, Training Accuracy: 0.8478, Validation Loss: 0.4873, Validation Accuracy: 0.8315
Epoch: 8, Training Loss: 0.4149, Training Accuracy: 0.8535, Validation Loss: 0.4780, Validation Accuracy: 0.8335
Epoch: 9, Training Loss: 0.3977, Training Accuracy: 0.8581, Validation Loss: 0.4708, Validation Accuracy: 0.8370
Epoch: 10, Training Loss: 0.3817, Training Accuracy: 0.8640, Validation Loss: 0.4648, Validation Accuracy: 0.8390
Epoch: 11, Training Loss: 0.3679, Training Accuracy: 0.8699, Validation Loss: 0.4598, Validation Accuracy: 0.8410
Epoch: 12, Training Loss: 0.3552, Training Accuracy: 0.8744, Validation Loss: 0.4539, Validation Accuracy: 0.8420
Epoch: 13, Training Loss: 0.3431, Training Accuracy: 0.8796, Validation Loss: 0.4488, Validation Accuracy: 0.8425
Epoch: 14, Training Loss: 0.3313, Training Accuracy: 0.8850, Validation Loss: 0.4544, Validation Accuracy: 0.8395
Epoch: 15, Training Loss: 0.3206, Training Accuracy: 0.8878, Validation Loss: 0.4436, Validation Accuracy: 0.8440
Epoch: 16, Training Loss: 0.3104, Training Accuracy: 0.8934, Validation Loss: 0.4440, Validation Accuracy: 0.8440
Epoch: 17, Training Loss: 0.3000, Training Accuracy: 0.8969, Validation Loss: 0.4410, Validation Accuracy: 0.8435
Epoch: 18, Training Loss: 0.2907, Training Accuracy: 0.9006, Validation Loss: 0.4401, Validation Accuracy: 0.8460
Epoch: 19, Training Loss: 0.2819, Training Accuracy: 0.9048, Validation Loss: 0.4371, Validation Accuracy: 0.8470
Epoch: 20, Training Loss: 0.2734, Training Accuracy: 0.9076, Validation Loss: 0.4372, Validation Accuracy: 0.8485
Epoch: 21, Training Loss: 0.2666, Training Accuracy: 0.9095, Validation Loss: 0.4330, Validation Accuracy: 0.8525
Epoch: 22, Training Loss: 0.2591, Training Accuracy: 0.9121, Validation Loss: 0.4313, Validation Accuracy: 0.8525
Epoch: 23, Training Loss: 0.2521, Training Accuracy: 0.9157, Validation Loss: 0.4292, Validation Accuracy: 0.8545
Epoch: 24, Training Loss: 0.2476, Training Accuracy: 0.9154, Validation Loss: 0.4242, Validation Accuracy: 0.8565
Epoch: 25, Training Loss: 0.2430, Training Accuracy: 0.9184, Validation Loss: 0.4251, Validation Accuracy: 0.8580
Epoch: 26, Training Loss: 0.2377, Training Accuracy: 0.9197, Validation Loss: 0.4314, Validation Accuracy: 0.8540
Epoch: 27, Training Loss: 0.2284, Training Accuracy: 0.9244, Validation Loss: 0.4431, Validation Accuracy: 0.8530
Epoch: 28, Training Loss: 0.2211, Training Accuracy: 0.9250, Validation Loss: 0.4521, Validation Accuracy: 0.8515
Epoch: 29, Training Loss: 0.2137, Training Accuracy: 0.9286, Validation Loss: 0.4597, Validation Accuracy: 0.8480
Epoch: 30, Training Loss: 0.2087, Training Accuracy: 0.9303, Validation Loss: 0.4627, Validation Accuracy: 0.8485
Epoch: 31, Training Loss: 0.2026, Training Accuracy: 0.9335, Validation Loss: 0.4625, Validation Accuracy: 0.8510
Epoch: 32, Training Loss: 0.1976, Training Accuracy: 0.9350, Validation Loss: 0.4600, Validation Accuracy: 0.8505
Epoch: 33, Training Loss: 0.1920, Training Accuracy: 0.9366, Validation Loss: 0.4597, Validation Accuracy: 0.8520
Epoch: 34, Training Loss: 0.1881, Training Accuracy: 0.9361, Validation Loss: 0.4593, Validation Accuracy: 0.8520
Epoch: 35, Training Loss: 0.1819, Training Accuracy: 0.9381, Validation Loss: 0.4612, Validation Accuracy: 0.8545
Epoch: 36, Training Loss: 0.1759, Training Accuracy: 0.9407, Validation Loss: 0.4622, Validation Accuracy: 0.8525
Epoch: 37, Training Loss: 0.1693, Training Accuracy: 0.9436, Validation Loss: 0.4678, Validation Accuracy: 0.8515
Epoch: 38, Training Loss: 0.1635, Training Accuracy: 0.9474, Validation Loss: 0.4683, Validation Accuracy: 0.8540
Epoch: 39, Training Loss: 0.1577, Training Accuracy: 0.9490, Validation Loss: 0.4730, Validation Accuracy: 0.8515
Epoch: 40, Training Loss: 0.1529, Training Accuracy: 0.9496, Validation Loss: 0.4725, Validation Accuracy: 0.8560
Epoch: 41, Training Loss: 0.1463, Training Accuracy: 0.9523, Validation Loss: 0.4754, Validation Accuracy: 0.8555
Epoch: 42, Training Loss: 0.1411, Training Accuracy: 0.9534, Validation Loss: 0.4796, Validation Accuracy: 0.8550
Epoch: 43, Training Loss: 0.1350, Training Accuracy: 0.9547, Validation Loss: 0.4855, Validation Accuracy: 0.8560
Epoch: 44, Training Loss: 0.1306, Training Accuracy: 0.9575, Validation Loss: 0.4859, Validation Accuracy: 0.8575
Epoch: 45, Training Loss: 0.1256, Training Accuracy: 0.9596, Validation Loss: 0.4925, Validation Accuracy: 0.8570
Epoch: 46, Training Loss: 0.1203, Training Accuracy: 0.9627, Validation Loss: 0.4976, Validation Accuracy: 0.8555
Epoch: 47, Training Loss: 0.1159, Training Accuracy: 0.9631, Validation Loss: 0.5004, Validation Accuracy: 0.8550
Epoch: 48, Training Loss: 0.1113, Training Accuracy: 0.9650, Validation Loss: 0.5059, Validation Accuracy: 0.8540
Epoch: 49, Training Loss: 0.1060, Training Accuracy: 0.9667, Validation Loss: 0.5137, Validation Accuracy: 0.8495
Epoch: 50, Training Loss: 0.1023, Training Accuracy: 0.9681, Validation Loss: 0.5209, Validation Accuracy: 0.8540
Epoch: 51, Training Loss: 0.0989, Training Accuracy: 0.9692, Validation Loss: 0.5281, Validation Accuracy: 0.8545
Epoch: 52, Training Loss: 0.0955, Training Accuracy: 0.9710, Validation Loss: 0.5373, Validation Accuracy: 0.8545
Epoch: 53, Training Loss: 0.0926, Training Accuracy: 0.9710, Validation Loss: 0.5509, Validation Accuracy: 0.8535
Epoch: 54, Training Loss: 0.0904, Training Accuracy: 0.9714, Validation Loss: 0.5617, Validation Accuracy: 0.8530
Epoch: 55, Training Loss: 0.0868, Training Accuracy: 0.9735, Validation Loss: 0.5706, Validation Accuracy: 0.8530
Epoch: 56, Training Loss: 0.0843, Training Accuracy: 0.9740, Validation Loss: 0.5790, Validation Accuracy: 0.8545
Epoch: 57, Training Loss: 0.0820, Training Accuracy: 0.9750, Validation Loss: 0.5936, Validation Accuracy: 0.8510
Epoch: 58, Training Loss: 0.0817, Training Accuracy: 0.9750, Validation Loss: 0.6018, Validation Accuracy: 0.8455
Epoch: 59, Training Loss: 0.0832, Training Accuracy: 0.9738, Validation Loss: 0.6124, Validation Accuracy: 0.8470
Epoch: 60, Training Loss: 0.0858, Training Accuracy: 0.9736, Validation Loss: 0.6296, Validation Accuracy: 0.8380
Epoch: 61, Training Loss: 0.0883, Training Accuracy: 0.9704, Validation Loss: 0.7387, Validation Accuracy: 0.8290
Epoch: 62, Training Loss: 0.0961, Training Accuracy: 0.9675, Validation Loss: 0.7836, Validation Accuracy: 0.8220
Epoch: 63, Training Loss: 0.0958, Training Accuracy: 0.9679, Validation Loss: 0.7014, Validation Accuracy: 0.8325
Epoch: 64, Training Loss: 0.0869, Training Accuracy: 0.9718, Validation Loss: 0.7434, Validation Accuracy: 0.8265
Epoch: 65, Training Loss: 0.0867, Training Accuracy: 0.9724, Validation Loss: 0.8247, Validation Accuracy: 0.8180
Epoch: 66, Training Loss: 0.0863, Training Accuracy: 0.9705, Validation Loss: 0.8833, Validation Accuracy: 0.8085
Epoch: 67, Training Loss: 0.0913, Training Accuracy: 0.9665, Validation Loss: 0.9358, Validation Accuracy: 0.8050
Epoch: 68, Training Loss: 0.1004, Training Accuracy: 0.9621, Validation Loss: 0.7635, Validation Accuracy: 0.8230
Epoch: 69, Training Loss: 0.0906, Training Accuracy: 0.9661, Validation Loss: 0.7434, Validation Accuracy: 0.8325
Epoch: 70, Training Loss: 0.0833, Training Accuracy: 0.9708, Validation Loss: 0.7150, Validation Accuracy: 0.8365
Epoch: 71, Training Loss: 0.0845, Training Accuracy: 0.9709, Validation Loss: 0.6873, Validation Accuracy: 0.8480
Epoch: 72, Training Loss: 0.0808, Training Accuracy: 0.9715, Validation Loss: 0.6656, Validation Accuracy: 0.8500
Epoch: 73, Training Loss: 0.0717, Training Accuracy: 0.9754, Validation Loss: 0.6633, Validation Accuracy: 0.8470
Epoch: 74, Training Loss: 0.0605, Training Accuracy: 0.9806, Validation Loss: 0.6857, Validation Accuracy: 0.8410
Epoch: 75, Training Loss: 0.0565, Training Accuracy: 0.9834, Validation Loss: 0.7063, Validation Accuracy: 0.8415
Epoch: 76, Training Loss: 0.0527, Training Accuracy: 0.9835, Validation Loss: 0.7370, Validation Accuracy: 0.8375
Epoch: 77, Training Loss: 0.0514, Training Accuracy: 0.9852, Validation Loss: 0.7416, Validation Accuracy: 0.8370
Epoch: 78, Training Loss: 0.0493, Training Accuracy: 0.9861, Validation Loss: 0.7716, Validation Accuracy: 0.8355
Epoch: 79, Training Loss: 0.0486, Training Accuracy: 0.9851, Validation Loss: 0.7831, Validation Accuracy: 0.8295
Epoch: 80, Training Loss: 0.0467, Training Accuracy: 0.9861, Validation Loss: 0.8089, Validation Accuracy: 0.8315
Epoch: 81, Training Loss: 0.0453, Training Accuracy: 0.9871, Validation Loss: 0.8237, Validation Accuracy: 0.8285
Epoch: 82, Training Loss: 0.0445, Training Accuracy: 0.9874, Validation Loss: 0.8237, Validation Accuracy: 0.8295
Epoch: 83, Training Loss: 0.0441, Training Accuracy: 0.9870, Validation Loss: 0.8471, Validation Accuracy: 0.8285
Epoch: 84, Training Loss: 0.0442, Training Accuracy: 0.9862, Validation Loss: 0.8393, Validation Accuracy: 0.8290
Epoch: 85, Training Loss: 0.0447, Training Accuracy: 0.9855, Validation Loss: 0.8334, Validation Accuracy: 0.8325
Epoch: 86, Training Loss: 0.0480, Training Accuracy: 0.9851, Validation Loss: 0.8660, Validation Accuracy: 0.8250
Epoch: 87, Training Loss: 0.0516, Training Accuracy: 0.9825, Validation Loss: 0.8666, Validation Accuracy: 0.8250
Epoch: 88, Training Loss: 0.0619, Training Accuracy: 0.9780, Validation Loss: 0.8544, Validation Accuracy: 0.8215
Epoch: 89, Training Loss: 0.0780, Training Accuracy: 0.9710, Validation Loss: 0.7760, Validation Accuracy: 0.8375
Epoch: 90, Training Loss: 0.0851, Training Accuracy: 0.9683, Validation Loss: 0.7082, Validation Accuracy: 0.8505
Epoch: 91, Training Loss: 0.0671, Training Accuracy: 0.9756, Validation Loss: 0.7170, Validation Accuracy: 0.8510
Epoch: 92, Training Loss: 0.0443, Training Accuracy: 0.9849, Validation Loss: 0.7531, Validation Accuracy: 0.8520
Epoch: 93, Training Loss: 0.0376, Training Accuracy: 0.9881, Validation Loss: 0.7708, Validation Accuracy: 0.8535
Epoch: 94, Training Loss: 0.0317, Training Accuracy: 0.9911, Validation Loss: 0.7508, Validation Accuracy: 0.8555
Epoch: 95, Training Loss: 0.0296, Training Accuracy: 0.9919, Validation Loss: 0.7520, Validation Accuracy: 0.8570
Epoch: 96, Training Loss: 0.0276, Training Accuracy: 0.9932, Validation Loss: 0.7572, Validation Accuracy: 0.8590
Epoch: 97, Training Loss: 0.0264, Training Accuracy: 0.9936, Validation Loss: 0.7770, Validation Accuracy: 0.8535
Epoch: 98, Training Loss: 0.0237, Training Accuracy: 0.9949, Validation Loss: 0.7856, Validation Accuracy: 0.8550
Epoch: 99, Training Loss: 0.0225, Training Accuracy: 0.9952, Validation Loss: 0.8080, Validation Accuracy: 0.8545
Epoch: 100, Training Loss: 0.0222, Training Accuracy: 0.9940, Validation Loss: 0.8126, Validation Accuracy: 0.8545

0.843
[57]
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(histsgd['eval_loss'], label='SGD')
plt.plot(histb['eval_loss'], label='RMSprop')
plt.plot(histadagrad['eval_loss'], label='Adagrad')
plt.plot(histadam['eval_loss'], label='Adam')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Loss')
plt.legend()
plt.show()
fig, ax = plt.subplots(figsize=(10, 8))
plt.plot(histsgd['eval_acc'], label='SGD')
plt.plot(histb['eval_acc'], label='RMSprop')
plt.plot(histadagrad['eval_acc'], label='Adagrad')
plt.plot(histadam['eval_acc'], label='Adam')
plt.xlabel('Epochs')
plt.ylabel('Evaluation Accuracy')
plt.legend()
plt.show()
[<matplotlib.lines.Line2D at 0x1a76dc93400>]
[<matplotlib.lines.Line2D at 0x1a76dc937f0>]
[<matplotlib.lines.Line2D at 0x1a76dc93af0>]
[<matplotlib.lines.Line2D at 0x1a76dc93d90>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Loss')
<matplotlib.legend.Legend at 0x1a76dc93d00>

[<matplotlib.lines.Line2D at 0x1a76dce3dc0>]
[<matplotlib.lines.Line2D at 0x1a76dcef0a0>]
[<matplotlib.lines.Line2D at 0x1a76dcef370>]
[<matplotlib.lines.Line2D at 0x1a76dcef640>]
Text(0.5, 0, 'Epochs')
Text(0, 0.5, 'Evaluation Accuracy')
<matplotlib.legend.Legend at 0x1a76dcef4f0>

[86]
# Plot learning curves

# Add your code

Test acc: SGD 0.657, Adagrad 0.861, RMSprop 0.866, Adam 0.861 


ANALYSIS
1-Based on the curve analysis, it looks that RMSProp works the best, followed by Adam, Adagrad, and SGD. This is most probable because both RMSProp and Adam adjust the learning rate, allowing them to converge faster and more precisely. Adagrad also adjusts the learning rate, but it is less effective in some circumstances due to the way gradients accumulate over time. SGD, on the other hand, has a fixed learning rate that does not adjust, which might make convergent optimization difficult, especially in noisy or high-variance data. 2- SGD is a popular optimization technique, although it usually performs poorly when compared to more advanced optimizers such as Adagrad, RMSProp, and Adam. The fundamental reason for this is that SGD has a set learning rate that applies to all parameters, making it difficult to converge on an optimal solution, especially if the data has a lot of noise or a large variation.
